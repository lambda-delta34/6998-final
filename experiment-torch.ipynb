{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "from torch.autograd import Variable\n",
    "from ada_hessian import AdaHessian\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 256\n",
    "\n",
    "\n",
    "def getNet(device):\n",
    "    net = models.resnet18()\n",
    "    net.to(device)\n",
    "    return net\n",
    "\n",
    "def generateExperiment(net, optimizer, trainloader, testloader, \n",
    "                       isHessian, csv_name, device, criterion = nn.CrossEntropyLoss(), total_epochs = 160):\n",
    "    scheduler = lr_scheduler.MultiStepLR(\n",
    "        optimizer,\n",
    "        [80, 120],\n",
    "        gamma=0.1,\n",
    "        last_epoch=-1)\n",
    "    train_losses = []\n",
    "    train_acc = []\n",
    "    train_times = []\n",
    "    val_loss = []\n",
    "    val_acc = []\n",
    "    epochs = []\n",
    "\n",
    "    for epoch in range(total_epochs):  \n",
    "\n",
    "        train_loss = 0.0\n",
    "        train_step = 0\n",
    "        train_total = 0\n",
    "        train_correct = 0\n",
    "        opt_time = 0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            train_step = train_step + 1\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            if isHessian:\n",
    "                loss.backward(create_graph=True)\n",
    "            else:\n",
    "                loss.backward()\n",
    "\n",
    "            t = time.process_time()\n",
    "            optimizer.step()\n",
    "            opt_time += time.process_time() - t\n",
    "\n",
    "            scheduler.step()\n",
    "\n",
    "            # print statistics\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        test_loss = 0.0\n",
    "        test_step = 0\n",
    "        test_total = 0\n",
    "        test_correct = 0\n",
    "\n",
    "        for i, data in enumerate(testloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            test_step = test_step + 1\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            test_total += labels.size(0)\n",
    "            test_correct += (predicted == labels).sum().item()\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            # print statistics\n",
    "            test_loss += loss.item()\n",
    "        train_losses.append(train_loss / train_step)\n",
    "        train_acc.append(train_correct / train_total)\n",
    "        train_times.append(opt_time / train_step)\n",
    "        val_loss.append(test_loss / test_step)\n",
    "        val_acc.append(test_correct / test_total)\n",
    "        epochs.append(epoch)\n",
    "        print(\"Epoch: \" + str(epoch) + \" finished\")\n",
    "    extract_dat = pd.DataFrame({\n",
    "        \"epoch\": epochs,\n",
    "        \"loss\": train_losses,\n",
    "        \"accuracy\": train_acc,\n",
    "        \"val_loss\": val_loss,\n",
    "        \"val_acc\": val_acc,\n",
    "        \"opt_time\": train_times,\n",
    "    })\n",
    "    extract_dat.to_csv(csv_name, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1: Computer Vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = getNet(device)\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "generateExperiment(net, optimizer, trainloader, testloader, False, \"SGD_Moment_torch.csv\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = getNet(device)\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.1, momentum=0, weight_decay=5e-4)\n",
    "\n",
    "generateExperiment(net, optimizer, trainloader, testloader, False, \"SGD_torch.csv\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = getNet(device)\n",
    "optimizer_ada = AdaHessian(net.parameters(), lr=0.15, \n",
    "                           average_conv_kernel=True, hessian_power=1, \n",
    "                           n_samples=1, weight_decay=5e-4)\n",
    "\n",
    "generateExperiment(net, optimizer_ada, trainloader, testloader, True, \"AdaHess_torch.csv\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = getNet(device)\n",
    "optimizer_adam = optim.Adam (net.parameters(), lr=0.001, weight_decay=5e-4)\n",
    "\n",
    "generateExperiment(net, optimizer_adam, trainloader, testloader, False, \"Adam_torch.csv\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = getNet(device)\n",
    "optimizer_adamw = optim.AdamW (net.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "generateExperiment(net, optimizer_adamw, trainloader, testloader, False, \"AdamW_torch.csv\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2: DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100000\n",
    "batch_size=2000\n",
    "def gen_egg_pts(n):\n",
    "    x1 = np.random.uniform(-512, 512, n)\n",
    "    x2 = np.random.uniform(-512, 512, n)\n",
    "    f_x = -(x2 + 47) * np.sin(np.sqrt(np.abs(x1 / 2 + (x2 + 47)))) \\\n",
    "        - x1 * np.sin(np.abs(x1 - (x2 + 47))) \n",
    "    noise = np.random.normal(0, math.sqrt(0.3), n) \n",
    "    X = np.transpose(np.array([x1, x2]))\n",
    "    return X, f_x + noise\n",
    "\n",
    "x, y = gen_egg_pts(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(n * 0.8)\n",
    "test_size = n - train_size\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "tensor_x = torch.Tensor(x)\n",
    "tensor_y = torch.Tensor(y)\n",
    "my_dataset = torch.utils.data.TensorDataset(tensor_x,tensor_y)\n",
    "\n",
    "train_reg, test_reg = torch.utils.data.random_split(my_dataset, (train_size, test_size))\n",
    "\n",
    "train_reg_loader = torch.utils.data.DataLoader(train_reg, batch_size = batch_size)\n",
    "test_reg_loader = torch.utils.data.DataLoader(test_reg, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reg_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Reg_Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(2, 120)\n",
    "        self.fc2 = nn.Linear(120, 120)\n",
    "        self.fc3 = nn.Linear(120, 120)\n",
    "        self.fc4 = nn.Linear(120, 120)\n",
    "        self.fc5 = nn.Linear(120, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        return x\n",
    "\n",
    "def getRegNet(device):\n",
    "    net = Reg_Net()\n",
    "    net.to(device)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateExperimentReg(net, optimizer, trainloader, testloader, \n",
    "                       isHessian, csv_name, device, criterion = nn.MSELoss(), total_epochs = 2000):\n",
    "    scheduler = lr_scheduler.MultiStepLR(\n",
    "        optimizer,\n",
    "        [800, 1200],\n",
    "        gamma=0.1,\n",
    "        last_epoch=-1)\n",
    "    train_losses = []\n",
    "    train_times = []\n",
    "    val_loss = []\n",
    "    epochs = []\n",
    "\n",
    "    for epoch in range(total_epochs):  \n",
    "\n",
    "        train_loss = 0.0\n",
    "        train_step = 0\n",
    "        train_total = 0\n",
    "        opt_time = 0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            train_step = train_step + 1\n",
    "            inputs, y = data[0].to(device), data[1].to(device)\n",
    "            y = torch.unsqueeze(y, 1)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "\n",
    "            train_total += y.size(0)\n",
    "\n",
    "            loss = criterion(outputs, y)\n",
    "            if isHessian:\n",
    "                loss.backward(create_graph=True)\n",
    "            else:\n",
    "                loss.backward()\n",
    "\n",
    "            t = time.process_time()\n",
    "            optimizer.step()\n",
    "            opt_time += time.process_time() - t\n",
    "\n",
    "            scheduler.step()\n",
    "\n",
    "            # print statistics\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        test_loss = 0.0\n",
    "        test_step = 0\n",
    "        test_total = 0\n",
    "\n",
    "        for i, data in enumerate(testloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            test_step = test_step + 1\n",
    "            inputs, y = data[0].to(device), data[1].to(device)\n",
    "            y = torch.unsqueeze(y, 1)\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            test_total += y.size(0)\n",
    "\n",
    "            loss = criterion(outputs, y)\n",
    "            # print statistics\n",
    "            test_loss += loss.item()\n",
    "        train_losses.append(train_loss / train_step)\n",
    "        train_times.append(opt_time / train_step)\n",
    "        val_loss.append(test_loss / test_step)\n",
    "        epochs.append(epoch)\n",
    "        print(\"Epoch: \" + str(epoch) + \" finished with training loss \" + str(train_loss / train_step))\n",
    "    extract_dat = pd.DataFrame({\n",
    "        \"epoch\": epochs,\n",
    "        \"loss\": train_losses,\n",
    "        \"val_loss\": val_loss,\n",
    "        \"opt_time\": train_times,\n",
    "    })\n",
    "    extract_dat.to_csv(csv_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = getRegNet(device)\n",
    "\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(), lr=10e-6, momentum=10e-3, weight_decay=5e-4)\n",
    "\n",
    "\n",
    "\n",
    "generateExperimentReg(net, optimizer, train_reg_loader, test_reg_loader, False, \n",
    "                      \"SGD_Moment_Reg_torch.csv\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = getRegNet(device)\n",
    "\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(), lr=10e-6, momentum=0, weight_decay=5e-4)\n",
    "\n",
    "\n",
    "\n",
    "generateExperimentReg(net, optimizer, train_reg_loader, test_reg_loader, False, \n",
    "                      \"SGD_Reg_torch.csv\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = getRegNet(device)\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), weight_decay=5e-4)\n",
    "\n",
    "\n",
    "\n",
    "generateExperimentReg(net, optimizer, train_reg_loader, test_reg_loader, False, \n",
    "                      \"Adam_Reg_torch.csv\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = getRegNet(device)\n",
    "\n",
    "\n",
    "optimizer = optim.AdamW(net.parameters(), weight_decay=5e-4)\n",
    "\n",
    "\n",
    "\n",
    "generateExperimentReg(net, optimizer, train_reg_loader, test_reg_loader, False, \n",
    "                      \"AdamW_Reg_torch.csv\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 finished with training loss 88842.07421875\n",
      "Epoch: 1 finished with training loss 85822.5548828125\n",
      "Epoch: 2 finished with training loss 84843.801953125\n",
      "Epoch: 3 finished with training loss 84437.1466796875\n",
      "Epoch: 4 finished with training loss 84201.66953125\n",
      "Epoch: 5 finished with training loss 84061.5443359375\n",
      "Epoch: 6 finished with training loss 83863.1912109375\n",
      "Epoch: 7 finished with training loss 83721.21171875\n",
      "Epoch: 8 finished with training loss 83549.837109375\n",
      "Epoch: 9 finished with training loss 83391.902734375\n",
      "Epoch: 10 finished with training loss 83292.856640625\n",
      "Epoch: 11 finished with training loss 83203.823828125\n",
      "Epoch: 12 finished with training loss 83167.7421875\n",
      "Epoch: 13 finished with training loss 83112.983203125\n",
      "Epoch: 14 finished with training loss 83049.5509765625\n",
      "Epoch: 15 finished with training loss 82906.5259765625\n",
      "Epoch: 16 finished with training loss 82696.4869140625\n",
      "Epoch: 17 finished with training loss 82642.173828125\n",
      "Epoch: 18 finished with training loss 82554.1890625\n",
      "Epoch: 19 finished with training loss 82488.441015625\n",
      "Epoch: 20 finished with training loss 82149.756640625\n",
      "Epoch: 21 finished with training loss 81981.971875\n",
      "Epoch: 22 finished with training loss 81946.524609375\n",
      "Epoch: 23 finished with training loss 81930.4828125\n",
      "Epoch: 24 finished with training loss 81913.295703125\n",
      "Epoch: 25 finished with training loss 81894.856640625\n",
      "Epoch: 26 finished with training loss 81880.262109375\n",
      "Epoch: 27 finished with training loss 81870.2783203125\n",
      "Epoch: 28 finished with training loss 81859.9712890625\n",
      "Epoch: 29 finished with training loss 81848.33203125\n",
      "Epoch: 30 finished with training loss 81783.7240234375\n",
      "Epoch: 31 finished with training loss 81771.126171875\n",
      "Epoch: 32 finished with training loss 81765.3763671875\n",
      "Epoch: 33 finished with training loss 81761.9419921875\n",
      "Epoch: 34 finished with training loss 81759.74140625\n",
      "Epoch: 35 finished with training loss 81758.0607421875\n",
      "Epoch: 36 finished with training loss 81756.728515625\n",
      "Epoch: 37 finished with training loss 81755.7046875\n",
      "Epoch: 38 finished with training loss 81754.42421875\n",
      "Epoch: 39 finished with training loss 81753.126953125\n",
      "Epoch: 40 finished with training loss 81752.0849609375\n",
      "Epoch: 41 finished with training loss 81750.8076171875\n",
      "Epoch: 42 finished with training loss 81749.5982421875\n",
      "Epoch: 43 finished with training loss 81748.235546875\n",
      "Epoch: 44 finished with training loss 81747.06796875\n",
      "Epoch: 45 finished with training loss 81745.8375\n",
      "Epoch: 46 finished with training loss 81744.841015625\n",
      "Epoch: 47 finished with training loss 81743.93671875\n",
      "Epoch: 48 finished with training loss 81743.1935546875\n",
      "Epoch: 49 finished with training loss 81742.2482421875\n",
      "Epoch: 50 finished with training loss 81741.1751953125\n",
      "Epoch: 51 finished with training loss 81740.4244140625\n",
      "Epoch: 52 finished with training loss 81739.5201171875\n",
      "Epoch: 53 finished with training loss 81738.52734375\n",
      "Epoch: 54 finished with training loss 81737.4861328125\n",
      "Epoch: 55 finished with training loss 81736.256640625\n",
      "Epoch: 56 finished with training loss 81735.1169921875\n",
      "Epoch: 57 finished with training loss 81734.163671875\n",
      "Epoch: 58 finished with training loss 81733.4388671875\n",
      "Epoch: 59 finished with training loss 81732.180859375\n",
      "Epoch: 60 finished with training loss 81730.920703125\n",
      "Epoch: 61 finished with training loss 81729.9408203125\n",
      "Epoch: 62 finished with training loss 81729.0833984375\n",
      "Epoch: 63 finished with training loss 81727.89375\n",
      "Epoch: 64 finished with training loss 81727.0955078125\n",
      "Epoch: 65 finished with training loss 81726.052734375\n",
      "Epoch: 66 finished with training loss 81725.1123046875\n",
      "Epoch: 67 finished with training loss 81723.9126953125\n",
      "Epoch: 68 finished with training loss 81722.89609375\n",
      "Epoch: 69 finished with training loss 81721.9314453125\n",
      "Epoch: 70 finished with training loss 81720.9703125\n",
      "Epoch: 71 finished with training loss 81719.9849609375\n",
      "Epoch: 72 finished with training loss 81718.6341796875\n",
      "Epoch: 73 finished with training loss 81717.44453125\n",
      "Epoch: 74 finished with training loss 81716.472265625\n",
      "Epoch: 75 finished with training loss 81715.79375\n",
      "Epoch: 76 finished with training loss 81714.096484375\n",
      "Epoch: 77 finished with training loss 81712.737890625\n",
      "Epoch: 78 finished with training loss 81711.5521484375\n",
      "Epoch: 79 finished with training loss 81710.4255859375\n",
      "Epoch: 80 finished with training loss 81709.1515625\n",
      "Epoch: 81 finished with training loss 81707.91640625\n",
      "Epoch: 82 finished with training loss 81706.4361328125\n",
      "Epoch: 83 finished with training loss 81705.02265625\n",
      "Epoch: 84 finished with training loss 81703.71640625\n",
      "Epoch: 85 finished with training loss 81702.3154296875\n",
      "Epoch: 86 finished with training loss 81701.0849609375\n",
      "Epoch: 87 finished with training loss 81699.52578125\n",
      "Epoch: 88 finished with training loss 81698.03125\n",
      "Epoch: 89 finished with training loss 81696.709375\n",
      "Epoch: 90 finished with training loss 81695.4865234375\n",
      "Epoch: 91 finished with training loss 81694.2125\n",
      "Epoch: 92 finished with training loss 81692.7017578125\n",
      "Epoch: 93 finished with training loss 81691.6310546875\n",
      "Epoch: 94 finished with training loss 81690.5521484375\n",
      "Epoch: 95 finished with training loss 81689.1912109375\n",
      "Epoch: 96 finished with training loss 81687.944140625\n",
      "Epoch: 97 finished with training loss 81686.9068359375\n",
      "Epoch: 98 finished with training loss 81685.48046875\n",
      "Epoch: 99 finished with training loss 81684.4853515625\n",
      "Epoch: 100 finished with training loss 81683.2955078125\n",
      "Epoch: 101 finished with training loss 81682.3185546875\n",
      "Epoch: 102 finished with training loss 81681.4900390625\n",
      "Epoch: 103 finished with training loss 81680.3296875\n",
      "Epoch: 104 finished with training loss 81679.16328125\n",
      "Epoch: 105 finished with training loss 81677.96875\n",
      "Epoch: 106 finished with training loss 81677.0951171875\n",
      "Epoch: 107 finished with training loss 81675.9416015625\n",
      "Epoch: 108 finished with training loss 81675.0142578125\n",
      "Epoch: 109 finished with training loss 81673.9716796875\n",
      "Epoch: 110 finished with training loss 81672.9458984375\n",
      "Epoch: 111 finished with training loss 81671.8060546875\n",
      "Epoch: 112 finished with training loss 81671.0681640625\n",
      "Epoch: 113 finished with training loss 81669.73828125\n",
      "Epoch: 114 finished with training loss 81668.73828125\n",
      "Epoch: 115 finished with training loss 81667.698828125\n",
      "Epoch: 116 finished with training loss 81666.8634765625\n",
      "Epoch: 117 finished with training loss 81665.774609375\n",
      "Epoch: 118 finished with training loss 81664.5390625\n",
      "Epoch: 119 finished with training loss 81663.4255859375\n",
      "Epoch: 120 finished with training loss 81662.6203125\n",
      "Epoch: 121 finished with training loss 81661.394921875\n",
      "Epoch: 122 finished with training loss 81659.832421875\n",
      "Epoch: 123 finished with training loss 81658.705859375\n",
      "Epoch: 124 finished with training loss 81657.974609375\n",
      "Epoch: 125 finished with training loss 81657.024609375\n",
      "Epoch: 126 finished with training loss 81656.0703125\n",
      "Epoch: 127 finished with training loss 81654.758984375\n",
      "Epoch: 128 finished with training loss 81653.6921875\n",
      "Epoch: 129 finished with training loss 81652.7419921875\n",
      "Epoch: 130 finished with training loss 81651.3201171875\n",
      "Epoch: 131 finished with training loss 81650.2705078125\n",
      "Epoch: 132 finished with training loss 81649.3005859375\n",
      "Epoch: 133 finished with training loss 81648.114453125\n",
      "Epoch: 134 finished with training loss 81647.1580078125\n",
      "Epoch: 135 finished with training loss 81646.3037109375\n",
      "Epoch: 136 finished with training loss 81644.9423828125\n",
      "Epoch: 137 finished with training loss 81643.964453125\n",
      "Epoch: 138 finished with training loss 81643.00625\n",
      "Epoch: 139 finished with training loss 81641.8515625\n",
      "Epoch: 140 finished with training loss 81640.6716796875\n",
      "Epoch: 141 finished with training loss 81639.7958984375\n",
      "Epoch: 142 finished with training loss 81639.1578125\n",
      "Epoch: 143 finished with training loss 81637.6728515625\n",
      "Epoch: 144 finished with training loss 81636.585546875\n",
      "Epoch: 145 finished with training loss 81635.5349609375\n",
      "Epoch: 146 finished with training loss 81634.2841796875\n",
      "Epoch: 147 finished with training loss 81633.2611328125\n",
      "Epoch: 148 finished with training loss 81632.272265625\n",
      "Epoch: 149 finished with training loss 81631.3400390625\n",
      "Epoch: 150 finished with training loss 81629.9912109375\n",
      "Epoch: 151 finished with training loss 81628.9140625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 152 finished with training loss 81627.7197265625\n",
      "Epoch: 153 finished with training loss 81626.724609375\n",
      "Epoch: 154 finished with training loss 81625.5734375\n",
      "Epoch: 155 finished with training loss 81624.608203125\n",
      "Epoch: 156 finished with training loss 81623.91015625\n",
      "Epoch: 157 finished with training loss 81622.8546875\n",
      "Epoch: 158 finished with training loss 81621.6400390625\n",
      "Epoch: 159 finished with training loss 81620.39296875\n",
      "Epoch: 160 finished with training loss 81619.2349609375\n",
      "Epoch: 161 finished with training loss 81618.3240234375\n",
      "Epoch: 162 finished with training loss 81617.325390625\n",
      "Epoch: 163 finished with training loss 81616.227734375\n",
      "Epoch: 164 finished with training loss 81615.2412109375\n",
      "Epoch: 165 finished with training loss 81614.18984375\n",
      "Epoch: 166 finished with training loss 81613.0525390625\n",
      "Epoch: 167 finished with training loss 81612.0439453125\n",
      "Epoch: 168 finished with training loss 81611.225390625\n",
      "Epoch: 169 finished with training loss 81610.3236328125\n",
      "Epoch: 170 finished with training loss 81609.37265625\n",
      "Epoch: 171 finished with training loss 81608.37890625\n",
      "Epoch: 172 finished with training loss 81607.4666015625\n",
      "Epoch: 173 finished with training loss 81606.412890625\n",
      "Epoch: 174 finished with training loss 81605.469140625\n",
      "Epoch: 175 finished with training loss 81604.5068359375\n",
      "Epoch: 176 finished with training loss 81603.1455078125\n",
      "Epoch: 177 finished with training loss 81602.18125\n",
      "Epoch: 178 finished with training loss 81601.2666015625\n",
      "Epoch: 179 finished with training loss 81600.3283203125\n",
      "Epoch: 180 finished with training loss 81599.4453125\n",
      "Epoch: 181 finished with training loss 81598.1548828125\n",
      "Epoch: 182 finished with training loss 81596.825390625\n",
      "Epoch: 183 finished with training loss 81595.871484375\n",
      "Epoch: 184 finished with training loss 81594.6533203125\n",
      "Epoch: 185 finished with training loss 81593.8865234375\n",
      "Epoch: 186 finished with training loss 81593.070703125\n",
      "Epoch: 187 finished with training loss 81592.505859375\n",
      "Epoch: 188 finished with training loss 81591.455859375\n",
      "Epoch: 189 finished with training loss 81590.3494140625\n",
      "Epoch: 190 finished with training loss 81589.5568359375\n",
      "Epoch: 191 finished with training loss 81588.6216796875\n",
      "Epoch: 192 finished with training loss 81587.6771484375\n",
      "Epoch: 193 finished with training loss 81586.7826171875\n",
      "Epoch: 194 finished with training loss 81585.787890625\n",
      "Epoch: 195 finished with training loss 81584.689453125\n",
      "Epoch: 196 finished with training loss 81583.7802734375\n",
      "Epoch: 197 finished with training loss 81582.7171875\n",
      "Epoch: 198 finished with training loss 81581.384765625\n",
      "Epoch: 199 finished with training loss 81580.4173828125\n",
      "Epoch: 200 finished with training loss 81579.4849609375\n",
      "Epoch: 201 finished with training loss 81578.709765625\n",
      "Epoch: 202 finished with training loss 81577.3931640625\n",
      "Epoch: 203 finished with training loss 81576.47421875\n",
      "Epoch: 204 finished with training loss 81575.57578125\n",
      "Epoch: 205 finished with training loss 81574.683203125\n",
      "Epoch: 206 finished with training loss 81573.8404296875\n",
      "Epoch: 207 finished with training loss 81572.940625\n",
      "Epoch: 208 finished with training loss 81571.9333984375\n",
      "Epoch: 209 finished with training loss 81571.053125\n",
      "Epoch: 210 finished with training loss 81570.1798828125\n",
      "Epoch: 211 finished with training loss 81569.1076171875\n",
      "Epoch: 212 finished with training loss 81568.257421875\n",
      "Epoch: 213 finished with training loss 81567.573046875\n",
      "Epoch: 214 finished with training loss 81566.7697265625\n",
      "Epoch: 215 finished with training loss 81565.778125\n",
      "Epoch: 216 finished with training loss 81564.8853515625\n",
      "Epoch: 217 finished with training loss 81563.9720703125\n",
      "Epoch: 218 finished with training loss 81563.1490234375\n",
      "Epoch: 219 finished with training loss 81562.3630859375\n",
      "Epoch: 220 finished with training loss 81561.2216796875\n",
      "Epoch: 221 finished with training loss 81560.2154296875\n",
      "Epoch: 222 finished with training loss 81559.7724609375\n",
      "Epoch: 223 finished with training loss 81559.051953125\n",
      "Epoch: 224 finished with training loss 81557.9171875\n",
      "Epoch: 225 finished with training loss 81557.353125\n",
      "Epoch: 226 finished with training loss 81556.5416015625\n",
      "Epoch: 227 finished with training loss 81555.439453125\n",
      "Epoch: 228 finished with training loss 81554.5078125\n",
      "Epoch: 229 finished with training loss 81553.6107421875\n",
      "Epoch: 230 finished with training loss 81552.8060546875\n",
      "Epoch: 231 finished with training loss 81551.824609375\n",
      "Epoch: 232 finished with training loss 81550.612109375\n",
      "Epoch: 233 finished with training loss 81549.750390625\n",
      "Epoch: 234 finished with training loss 81549.0453125\n",
      "Epoch: 235 finished with training loss 81548.4984375\n",
      "Epoch: 236 finished with training loss 81547.573046875\n",
      "Epoch: 237 finished with training loss 81546.43515625\n",
      "Epoch: 238 finished with training loss 81545.7263671875\n",
      "Epoch: 239 finished with training loss 81544.68828125\n",
      "Epoch: 240 finished with training loss 81543.969921875\n",
      "Epoch: 241 finished with training loss 81543.2822265625\n",
      "Epoch: 242 finished with training loss 81542.219921875\n",
      "Epoch: 243 finished with training loss 81541.22421875\n",
      "Epoch: 244 finished with training loss 81540.2013671875\n",
      "Epoch: 245 finished with training loss 81539.3021484375\n",
      "Epoch: 246 finished with training loss 81538.4767578125\n",
      "Epoch: 247 finished with training loss 81537.6826171875\n",
      "Epoch: 248 finished with training loss 81536.8\n",
      "Epoch: 249 finished with training loss 81535.8865234375\n",
      "Epoch: 250 finished with training loss 81535.0017578125\n",
      "Epoch: 251 finished with training loss 81534.420703125\n",
      "Epoch: 252 finished with training loss 81533.635546875\n",
      "Epoch: 253 finished with training loss 81533.119140625\n",
      "Epoch: 254 finished with training loss 81532.0708984375\n",
      "Epoch: 255 finished with training loss 81531.302734375\n",
      "Epoch: 256 finished with training loss 81530.5978515625\n",
      "Epoch: 257 finished with training loss 81529.6326171875\n",
      "Epoch: 258 finished with training loss 81528.725\n",
      "Epoch: 259 finished with training loss 81527.9771484375\n",
      "Epoch: 260 finished with training loss 81527.037890625\n",
      "Epoch: 261 finished with training loss 81526.4345703125\n",
      "Epoch: 262 finished with training loss 81525.542578125\n",
      "Epoch: 263 finished with training loss 81524.8021484375\n",
      "Epoch: 264 finished with training loss 81524.0166015625\n",
      "Epoch: 265 finished with training loss 81523.3544921875\n",
      "Epoch: 266 finished with training loss 81522.4529296875\n",
      "Epoch: 267 finished with training loss 81521.7095703125\n",
      "Epoch: 268 finished with training loss 81520.916796875\n",
      "Epoch: 269 finished with training loss 81520.282421875\n",
      "Epoch: 270 finished with training loss 81519.6984375\n",
      "Epoch: 271 finished with training loss 81519.0587890625\n",
      "Epoch: 272 finished with training loss 81518.53046875\n",
      "Epoch: 273 finished with training loss 81517.6423828125\n",
      "Epoch: 274 finished with training loss 81516.87578125\n",
      "Epoch: 275 finished with training loss 81515.801171875\n",
      "Epoch: 276 finished with training loss 81515.0931640625\n",
      "Epoch: 277 finished with training loss 81514.410546875\n",
      "Epoch: 278 finished with training loss 81513.7482421875\n",
      "Epoch: 279 finished with training loss 81513.03359375\n",
      "Epoch: 280 finished with training loss 81512.286328125\n",
      "Epoch: 281 finished with training loss 81511.3146484375\n",
      "Epoch: 282 finished with training loss 81510.7474609375\n",
      "Epoch: 283 finished with training loss 81509.9787109375\n",
      "Epoch: 284 finished with training loss 81509.156640625\n",
      "Epoch: 285 finished with training loss 81508.39453125\n",
      "Epoch: 286 finished with training loss 81507.958984375\n",
      "Epoch: 287 finished with training loss 81507.1455078125\n",
      "Epoch: 288 finished with training loss 81506.652734375\n",
      "Epoch: 289 finished with training loss 81505.86640625\n",
      "Epoch: 290 finished with training loss 81505.3751953125\n",
      "Epoch: 291 finished with training loss 81504.512109375\n",
      "Epoch: 292 finished with training loss 81504.2552734375\n",
      "Epoch: 293 finished with training loss 81503.282421875\n",
      "Epoch: 294 finished with training loss 81502.670703125\n",
      "Epoch: 295 finished with training loss 81501.9232421875\n",
      "Epoch: 296 finished with training loss 81501.065234375\n",
      "Epoch: 297 finished with training loss 81500.3251953125\n",
      "Epoch: 298 finished with training loss 81499.5529296875\n",
      "Epoch: 299 finished with training loss 81498.9498046875\n",
      "Epoch: 300 finished with training loss 81498.0283203125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 301 finished with training loss 81497.5220703125\n",
      "Epoch: 302 finished with training loss 81496.543359375\n",
      "Epoch: 303 finished with training loss 81496.028515625\n",
      "Epoch: 304 finished with training loss 81495.51328125\n",
      "Epoch: 305 finished with training loss 81494.8802734375\n",
      "Epoch: 306 finished with training loss 81494.2802734375\n",
      "Epoch: 307 finished with training loss 81493.5431640625\n",
      "Epoch: 308 finished with training loss 81492.92109375\n",
      "Epoch: 309 finished with training loss 81492.0115234375\n",
      "Epoch: 310 finished with training loss 81490.846875\n",
      "Epoch: 311 finished with training loss 81490.232421875\n",
      "Epoch: 312 finished with training loss 81489.67421875\n",
      "Epoch: 313 finished with training loss 81489.0880859375\n",
      "Epoch: 314 finished with training loss 81488.441796875\n",
      "Epoch: 315 finished with training loss 81487.9869140625\n",
      "Epoch: 316 finished with training loss 81487.2419921875\n",
      "Epoch: 317 finished with training loss 81486.89140625\n",
      "Epoch: 318 finished with training loss 81486.4380859375\n",
      "Epoch: 319 finished with training loss 81485.4248046875\n",
      "Epoch: 320 finished with training loss 81484.8140625\n",
      "Epoch: 321 finished with training loss 81484.075390625\n",
      "Epoch: 322 finished with training loss 81483.5583984375\n",
      "Epoch: 323 finished with training loss 81483.2029296875\n",
      "Epoch: 324 finished with training loss 81482.618359375\n",
      "Epoch: 325 finished with training loss 81481.9830078125\n",
      "Epoch: 326 finished with training loss 81481.54296875\n",
      "Epoch: 327 finished with training loss 81480.976953125\n",
      "Epoch: 328 finished with training loss 81480.550390625\n",
      "Epoch: 329 finished with training loss 81480.01875\n",
      "Epoch: 330 finished with training loss 81479.4603515625\n",
      "Epoch: 331 finished with training loss 81478.7115234375\n",
      "Epoch: 332 finished with training loss 81478.1701171875\n",
      "Epoch: 333 finished with training loss 81477.36484375\n",
      "Epoch: 334 finished with training loss 81476.89921875\n",
      "Epoch: 335 finished with training loss 81476.2740234375\n",
      "Epoch: 336 finished with training loss 81475.675390625\n",
      "Epoch: 337 finished with training loss 81475.2662109375\n",
      "Epoch: 338 finished with training loss 81474.3291015625\n",
      "Epoch: 339 finished with training loss 81474.0958984375\n",
      "Epoch: 340 finished with training loss 81473.4623046875\n",
      "Epoch: 341 finished with training loss 81472.7251953125\n",
      "Epoch: 342 finished with training loss 81472.4544921875\n",
      "Epoch: 343 finished with training loss 81471.7986328125\n",
      "Epoch: 344 finished with training loss 81471.1388671875\n",
      "Epoch: 345 finished with training loss 81471.0173828125\n",
      "Epoch: 346 finished with training loss 81470.21640625\n",
      "Epoch: 347 finished with training loss 81469.8880859375\n",
      "Epoch: 348 finished with training loss 81469.4462890625\n",
      "Epoch: 349 finished with training loss 81468.4265625\n",
      "Epoch: 350 finished with training loss 81467.8302734375\n",
      "Epoch: 351 finished with training loss 81467.3681640625\n",
      "Epoch: 352 finished with training loss 81466.805859375\n",
      "Epoch: 353 finished with training loss 81466.3240234375\n",
      "Epoch: 354 finished with training loss 81465.6087890625\n",
      "Epoch: 355 finished with training loss 81465.0677734375\n",
      "Epoch: 356 finished with training loss 81464.75390625\n",
      "Epoch: 357 finished with training loss 81464.1634765625\n",
      "Epoch: 358 finished with training loss 81463.4208984375\n",
      "Epoch: 359 finished with training loss 81463.1291015625\n",
      "Epoch: 360 finished with training loss 81462.7326171875\n",
      "Epoch: 361 finished with training loss 81462.2767578125\n",
      "Epoch: 362 finished with training loss 81461.8853515625\n",
      "Epoch: 363 finished with training loss 81461.2029296875\n",
      "Epoch: 364 finished with training loss 81460.6162109375\n",
      "Epoch: 365 finished with training loss 81460.569140625\n",
      "Epoch: 366 finished with training loss 81460.3232421875\n",
      "Epoch: 367 finished with training loss 81459.508203125\n",
      "Epoch: 368 finished with training loss 81459.07109375\n",
      "Epoch: 369 finished with training loss 81458.6099609375\n",
      "Epoch: 370 finished with training loss 81457.9841796875\n",
      "Epoch: 371 finished with training loss 81458.0173828125\n",
      "Epoch: 372 finished with training loss 81457.2751953125\n",
      "Epoch: 373 finished with training loss 81456.633203125\n",
      "Epoch: 374 finished with training loss 81456.2947265625\n",
      "Epoch: 375 finished with training loss 81455.681640625\n",
      "Epoch: 376 finished with training loss 81455.2140625\n",
      "Epoch: 377 finished with training loss 81454.6939453125\n",
      "Epoch: 378 finished with training loss 81454.236328125\n",
      "Epoch: 379 finished with training loss 81453.9373046875\n",
      "Epoch: 380 finished with training loss 81453.33828125\n",
      "Epoch: 381 finished with training loss 81452.95625\n",
      "Epoch: 382 finished with training loss 81452.753125\n",
      "Epoch: 383 finished with training loss 81452.205859375\n",
      "Epoch: 384 finished with training loss 81451.79765625\n",
      "Epoch: 385 finished with training loss 81451.3427734375\n",
      "Epoch: 386 finished with training loss 81451.1076171875\n",
      "Epoch: 387 finished with training loss 81450.508203125\n",
      "Epoch: 388 finished with training loss 81450.055859375\n",
      "Epoch: 389 finished with training loss 81449.5587890625\n",
      "Epoch: 390 finished with training loss 81449.2740234375\n",
      "Epoch: 391 finished with training loss 81448.8267578125\n",
      "Epoch: 392 finished with training loss 81448.3072265625\n",
      "Epoch: 393 finished with training loss 81447.6990234375\n",
      "Epoch: 394 finished with training loss 81447.085546875\n",
      "Epoch: 395 finished with training loss 81446.5453125\n",
      "Epoch: 396 finished with training loss 81446.02265625\n",
      "Epoch: 397 finished with training loss 81445.7421875\n",
      "Epoch: 398 finished with training loss 81445.1728515625\n",
      "Epoch: 399 finished with training loss 81444.79140625\n",
      "Epoch: 400 finished with training loss 81444.5447265625\n",
      "Epoch: 401 finished with training loss 81444.186328125\n",
      "Epoch: 402 finished with training loss 81443.9015625\n",
      "Epoch: 403 finished with training loss 81443.4537109375\n",
      "Epoch: 404 finished with training loss 81443.2796875\n",
      "Epoch: 405 finished with training loss 81442.9126953125\n",
      "Epoch: 406 finished with training loss 81442.425\n",
      "Epoch: 407 finished with training loss 81441.9978515625\n",
      "Epoch: 408 finished with training loss 81441.5873046875\n",
      "Epoch: 409 finished with training loss 81441.291015625\n",
      "Epoch: 410 finished with training loss 81440.7109375\n",
      "Epoch: 411 finished with training loss 81440.4083984375\n",
      "Epoch: 412 finished with training loss 81439.9048828125\n",
      "Epoch: 413 finished with training loss 81439.5392578125\n",
      "Epoch: 414 finished with training loss 81439.14140625\n",
      "Epoch: 415 finished with training loss 81438.440234375\n",
      "Epoch: 416 finished with training loss 81438.186328125\n",
      "Epoch: 417 finished with training loss 81437.798046875\n",
      "Epoch: 418 finished with training loss 81437.945703125\n",
      "Epoch: 419 finished with training loss 81437.2724609375\n",
      "Epoch: 420 finished with training loss 81437.007421875\n",
      "Epoch: 421 finished with training loss 81436.3166015625\n",
      "Epoch: 422 finished with training loss 81435.919921875\n",
      "Epoch: 423 finished with training loss 81435.7376953125\n",
      "Epoch: 424 finished with training loss 81435.1181640625\n",
      "Epoch: 425 finished with training loss 81435.056640625\n",
      "Epoch: 426 finished with training loss 81434.5228515625\n",
      "Epoch: 427 finished with training loss 81434.0994140625\n",
      "Epoch: 428 finished with training loss 81433.9099609375\n",
      "Epoch: 429 finished with training loss 81433.4265625\n",
      "Epoch: 430 finished with training loss 81433.17265625\n",
      "Epoch: 431 finished with training loss 81432.5685546875\n",
      "Epoch: 432 finished with training loss 81432.149609375\n",
      "Epoch: 433 finished with training loss 81432.12890625\n",
      "Epoch: 434 finished with training loss 81431.5625\n",
      "Epoch: 435 finished with training loss 81431.04296875\n",
      "Epoch: 436 finished with training loss 81430.5\n",
      "Epoch: 437 finished with training loss 81430.14453125\n",
      "Epoch: 438 finished with training loss 81429.6728515625\n",
      "Epoch: 439 finished with training loss 81429.2005859375\n",
      "Epoch: 440 finished with training loss 81428.8865234375\n",
      "Epoch: 441 finished with training loss 81428.3875\n",
      "Epoch: 442 finished with training loss 81428.0931640625\n",
      "Epoch: 443 finished with training loss 81427.6443359375\n",
      "Epoch: 444 finished with training loss 81427.2955078125\n",
      "Epoch: 445 finished with training loss 81426.780078125\n",
      "Epoch: 446 finished with training loss 81426.539453125\n",
      "Epoch: 447 finished with training loss 81425.9876953125\n",
      "Epoch: 448 finished with training loss 81425.6869140625\n",
      "Epoch: 449 finished with training loss 81425.3126953125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 450 finished with training loss 81425.0037109375\n",
      "Epoch: 451 finished with training loss 81424.7373046875\n",
      "Epoch: 452 finished with training loss 81424.39375\n",
      "Epoch: 453 finished with training loss 81424.0369140625\n",
      "Epoch: 454 finished with training loss 81423.6953125\n",
      "Epoch: 455 finished with training loss 81423.1986328125\n",
      "Epoch: 456 finished with training loss 81422.838671875\n",
      "Epoch: 457 finished with training loss 81422.4423828125\n",
      "Epoch: 458 finished with training loss 81422.3087890625\n",
      "Epoch: 459 finished with training loss 81421.790234375\n",
      "Epoch: 460 finished with training loss 81421.5095703125\n",
      "Epoch: 461 finished with training loss 81421.2388671875\n",
      "Epoch: 462 finished with training loss 81420.95\n",
      "Epoch: 463 finished with training loss 81420.5796875\n",
      "Epoch: 464 finished with training loss 81420.150390625\n",
      "Epoch: 465 finished with training loss 81419.9240234375\n",
      "Epoch: 466 finished with training loss 81419.4720703125\n",
      "Epoch: 467 finished with training loss 81419.322265625\n",
      "Epoch: 468 finished with training loss 81419.015625\n",
      "Epoch: 469 finished with training loss 81418.476953125\n",
      "Epoch: 470 finished with training loss 81418.1162109375\n",
      "Epoch: 471 finished with training loss 81417.76328125\n",
      "Epoch: 472 finished with training loss 81417.321875\n",
      "Epoch: 473 finished with training loss 81416.8916015625\n",
      "Epoch: 474 finished with training loss 81416.5349609375\n",
      "Epoch: 475 finished with training loss 81415.9708984375\n",
      "Epoch: 476 finished with training loss 81415.8005859375\n",
      "Epoch: 477 finished with training loss 81415.6013671875\n",
      "Epoch: 478 finished with training loss 81415.4517578125\n",
      "Epoch: 479 finished with training loss 81414.9990234375\n",
      "Epoch: 480 finished with training loss 81414.5064453125\n",
      "Epoch: 481 finished with training loss 81414.0787109375\n",
      "Epoch: 482 finished with training loss 81413.7388671875\n",
      "Epoch: 483 finished with training loss 81413.6294921875\n",
      "Epoch: 484 finished with training loss 81413.45859375\n",
      "Epoch: 485 finished with training loss 81412.8494140625\n",
      "Epoch: 486 finished with training loss 81412.5779296875\n",
      "Epoch: 487 finished with training loss 81412.5970703125\n",
      "Epoch: 488 finished with training loss 81412.1185546875\n",
      "Epoch: 489 finished with training loss 81411.615625\n",
      "Epoch: 490 finished with training loss 81411.2248046875\n",
      "Epoch: 491 finished with training loss 81411.0806640625\n",
      "Epoch: 492 finished with training loss 81410.633203125\n",
      "Epoch: 493 finished with training loss 81410.3228515625\n",
      "Epoch: 494 finished with training loss 81410.0353515625\n",
      "Epoch: 495 finished with training loss 81409.615625\n",
      "Epoch: 496 finished with training loss 81409.5599609375\n",
      "Epoch: 497 finished with training loss 81408.865234375\n",
      "Epoch: 498 finished with training loss 81408.62265625\n",
      "Epoch: 499 finished with training loss 81408.2771484375\n",
      "Epoch: 500 finished with training loss 81408.2064453125\n",
      "Epoch: 501 finished with training loss 81407.4259765625\n",
      "Epoch: 502 finished with training loss 81407.203125\n",
      "Epoch: 503 finished with training loss 81407.0892578125\n",
      "Epoch: 504 finished with training loss 81406.671875\n",
      "Epoch: 505 finished with training loss 81406.50546875\n",
      "Epoch: 506 finished with training loss 81406.3115234375\n",
      "Epoch: 507 finished with training loss 81405.8208984375\n",
      "Epoch: 508 finished with training loss 81405.484765625\n",
      "Epoch: 509 finished with training loss 81404.940625\n",
      "Epoch: 510 finished with training loss 81403.6193359375\n",
      "Epoch: 511 finished with training loss 81403.3884765625\n",
      "Epoch: 512 finished with training loss 81402.9671875\n",
      "Epoch: 513 finished with training loss 81402.6279296875\n",
      "Epoch: 514 finished with training loss 81402.3259765625\n",
      "Epoch: 515 finished with training loss 81402.0341796875\n",
      "Epoch: 516 finished with training loss 81401.8802734375\n",
      "Epoch: 517 finished with training loss 81401.59453125\n",
      "Epoch: 518 finished with training loss 81401.1689453125\n",
      "Epoch: 519 finished with training loss 81401.2259765625\n",
      "Epoch: 520 finished with training loss 81400.72421875\n",
      "Epoch: 521 finished with training loss 81400.337890625\n",
      "Epoch: 522 finished with training loss 81400.0134765625\n",
      "Epoch: 523 finished with training loss 81399.672265625\n",
      "Epoch: 524 finished with training loss 81399.313671875\n",
      "Epoch: 525 finished with training loss 81399.0212890625\n",
      "Epoch: 526 finished with training loss 81398.8091796875\n",
      "Epoch: 527 finished with training loss 81398.122265625\n",
      "Epoch: 528 finished with training loss 81397.8591796875\n",
      "Epoch: 529 finished with training loss 81397.551171875\n",
      "Epoch: 530 finished with training loss 81397.29296875\n",
      "Epoch: 531 finished with training loss 81397.214453125\n",
      "Epoch: 532 finished with training loss 81396.977734375\n",
      "Epoch: 533 finished with training loss 81396.3849609375\n",
      "Epoch: 534 finished with training loss 81396.0919921875\n",
      "Epoch: 535 finished with training loss 81395.62421875\n",
      "Epoch: 536 finished with training loss 81395.206640625\n",
      "Epoch: 537 finished with training loss 81394.92109375\n",
      "Epoch: 538 finished with training loss 81394.594140625\n",
      "Epoch: 539 finished with training loss 81394.2810546875\n",
      "Epoch: 540 finished with training loss 81393.9693359375\n",
      "Epoch: 541 finished with training loss 81393.6373046875\n",
      "Epoch: 542 finished with training loss 81393.4021484375\n",
      "Epoch: 543 finished with training loss 81393.2857421875\n",
      "Epoch: 544 finished with training loss 81392.6787109375\n",
      "Epoch: 545 finished with training loss 81392.6197265625\n",
      "Epoch: 546 finished with training loss 81392.267578125\n",
      "Epoch: 547 finished with training loss 81392.003515625\n",
      "Epoch: 548 finished with training loss 81391.617578125\n",
      "Epoch: 549 finished with training loss 81391.6599609375\n",
      "Epoch: 550 finished with training loss 81391.5056640625\n",
      "Epoch: 551 finished with training loss 81390.969921875\n",
      "Epoch: 552 finished with training loss 81390.79921875\n",
      "Epoch: 553 finished with training loss 81389.43125\n",
      "Epoch: 554 finished with training loss 81388.5599609375\n",
      "Epoch: 555 finished with training loss 81388.1869140625\n",
      "Epoch: 556 finished with training loss 81387.6009765625\n",
      "Epoch: 557 finished with training loss 81387.2248046875\n",
      "Epoch: 558 finished with training loss 81386.8291015625\n",
      "Epoch: 559 finished with training loss 81386.122265625\n",
      "Epoch: 560 finished with training loss 81386.13671875\n",
      "Epoch: 561 finished with training loss 81385.821875\n",
      "Epoch: 562 finished with training loss 81385.72109375\n",
      "Epoch: 563 finished with training loss 81385.25859375\n",
      "Epoch: 564 finished with training loss 81385.021875\n",
      "Epoch: 565 finished with training loss 81384.73671875\n",
      "Epoch: 566 finished with training loss 81384.2076171875\n",
      "Epoch: 567 finished with training loss 81384.1443359375\n",
      "Epoch: 568 finished with training loss 81383.8498046875\n",
      "Epoch: 569 finished with training loss 81383.5298828125\n",
      "Epoch: 570 finished with training loss 81383.1765625\n",
      "Epoch: 571 finished with training loss 81382.85234375\n",
      "Epoch: 572 finished with training loss 81382.532421875\n",
      "Epoch: 573 finished with training loss 81382.0634765625\n",
      "Epoch: 574 finished with training loss 81381.5828125\n",
      "Epoch: 575 finished with training loss 81381.21484375\n",
      "Epoch: 576 finished with training loss 81380.8869140625\n",
      "Epoch: 577 finished with training loss 81380.671875\n",
      "Epoch: 578 finished with training loss 81380.4751953125\n",
      "Epoch: 579 finished with training loss 81380.063671875\n",
      "Epoch: 580 finished with training loss 81379.737890625\n",
      "Epoch: 581 finished with training loss 81379.3732421875\n",
      "Epoch: 582 finished with training loss 81379.2455078125\n",
      "Epoch: 583 finished with training loss 81378.9701171875\n",
      "Epoch: 584 finished with training loss 81378.873828125\n",
      "Epoch: 585 finished with training loss 81378.3779296875\n",
      "Epoch: 586 finished with training loss 81378.0908203125\n",
      "Epoch: 587 finished with training loss 81377.8359375\n",
      "Epoch: 588 finished with training loss 81377.55625\n",
      "Epoch: 589 finished with training loss 81377.0537109375\n",
      "Epoch: 590 finished with training loss 81376.7490234375\n",
      "Epoch: 591 finished with training loss 81376.6501953125\n",
      "Epoch: 592 finished with training loss 81376.1912109375\n",
      "Epoch: 593 finished with training loss 81375.7306640625\n",
      "Epoch: 594 finished with training loss 81375.5359375\n",
      "Epoch: 595 finished with training loss 81375.2306640625\n",
      "Epoch: 596 finished with training loss 81374.8361328125\n",
      "Epoch: 597 finished with training loss 81374.8404296875\n",
      "Epoch: 598 finished with training loss 81374.309765625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 599 finished with training loss 81373.956640625\n",
      "Epoch: 600 finished with training loss 81374.0306640625\n",
      "Epoch: 601 finished with training loss 81373.800390625\n",
      "Epoch: 602 finished with training loss 81373.412890625\n",
      "Epoch: 603 finished with training loss 81373.3458984375\n",
      "Epoch: 604 finished with training loss 81372.937109375\n",
      "Epoch: 605 finished with training loss 81372.5498046875\n",
      "Epoch: 606 finished with training loss 81372.4931640625\n",
      "Epoch: 607 finished with training loss 81372.18125\n",
      "Epoch: 608 finished with training loss 81371.8869140625\n",
      "Epoch: 609 finished with training loss 81371.48828125\n",
      "Epoch: 610 finished with training loss 81371.3447265625\n",
      "Epoch: 611 finished with training loss 81371.230859375\n",
      "Epoch: 612 finished with training loss 81371.0005859375\n",
      "Epoch: 613 finished with training loss 81370.467578125\n",
      "Epoch: 614 finished with training loss 81370.507421875\n",
      "Epoch: 615 finished with training loss 81370.194921875\n",
      "Epoch: 616 finished with training loss 81369.7462890625\n",
      "Epoch: 617 finished with training loss 81369.55859375\n",
      "Epoch: 618 finished with training loss 81369.2123046875\n",
      "Epoch: 619 finished with training loss 81369.0271484375\n",
      "Epoch: 620 finished with training loss 81368.6044921875\n",
      "Epoch: 621 finished with training loss 81368.3458984375\n",
      "Epoch: 622 finished with training loss 81367.9912109375\n",
      "Epoch: 623 finished with training loss 81367.919140625\n",
      "Epoch: 624 finished with training loss 81367.62421875\n",
      "Epoch: 625 finished with training loss 81367.1544921875\n",
      "Epoch: 626 finished with training loss 81366.5896484375\n",
      "Epoch: 627 finished with training loss 81366.680078125\n",
      "Epoch: 628 finished with training loss 81366.283984375\n",
      "Epoch: 629 finished with training loss 81366.227734375\n",
      "Epoch: 630 finished with training loss 81365.9154296875\n",
      "Epoch: 631 finished with training loss 81365.4083984375\n",
      "Epoch: 632 finished with training loss 81365.252734375\n",
      "Epoch: 633 finished with training loss 81364.75859375\n",
      "Epoch: 634 finished with training loss 81364.7171875\n",
      "Epoch: 635 finished with training loss 81364.3744140625\n",
      "Epoch: 636 finished with training loss 81363.80390625\n",
      "Epoch: 637 finished with training loss 81363.68984375\n",
      "Epoch: 638 finished with training loss 81363.56328125\n",
      "Epoch: 639 finished with training loss 81363.432421875\n",
      "Epoch: 640 finished with training loss 81363.117578125\n",
      "Epoch: 641 finished with training loss 81362.912109375\n",
      "Epoch: 642 finished with training loss 81362.6955078125\n",
      "Epoch: 643 finished with training loss 81362.3171875\n",
      "Epoch: 644 finished with training loss 81362.133984375\n",
      "Epoch: 645 finished with training loss 81361.5986328125\n",
      "Epoch: 646 finished with training loss 81361.51953125\n",
      "Epoch: 647 finished with training loss 81361.144140625\n",
      "Epoch: 648 finished with training loss 81360.925390625\n",
      "Epoch: 649 finished with training loss 81360.7267578125\n",
      "Epoch: 650 finished with training loss 81360.5158203125\n",
      "Epoch: 651 finished with training loss 81360.2796875\n",
      "Epoch: 652 finished with training loss 81360.2212890625\n",
      "Epoch: 653 finished with training loss 81359.7162109375\n",
      "Epoch: 654 finished with training loss 81359.2806640625\n",
      "Epoch: 655 finished with training loss 81359.24140625\n",
      "Epoch: 656 finished with training loss 81358.7869140625\n",
      "Epoch: 657 finished with training loss 81358.840625\n",
      "Epoch: 658 finished with training loss 81358.498828125\n",
      "Epoch: 659 finished with training loss 81358.079296875\n",
      "Epoch: 660 finished with training loss 81357.8650390625\n",
      "Epoch: 661 finished with training loss 81357.549609375\n",
      "Epoch: 662 finished with training loss 81357.129296875\n",
      "Epoch: 663 finished with training loss 81356.9177734375\n",
      "Epoch: 664 finished with training loss 81356.765625\n",
      "Epoch: 665 finished with training loss 81356.219921875\n",
      "Epoch: 666 finished with training loss 81356.10859375\n",
      "Epoch: 667 finished with training loss 81355.9775390625\n",
      "Epoch: 668 finished with training loss 81355.5298828125\n",
      "Epoch: 669 finished with training loss 81355.2908203125\n",
      "Epoch: 670 finished with training loss 81354.921484375\n",
      "Epoch: 671 finished with training loss 81354.7841796875\n",
      "Epoch: 672 finished with training loss 81354.4947265625\n",
      "Epoch: 673 finished with training loss 81354.4439453125\n",
      "Epoch: 674 finished with training loss 81354.244140625\n",
      "Epoch: 675 finished with training loss 81354.0603515625\n",
      "Epoch: 676 finished with training loss 81353.5169921875\n",
      "Epoch: 677 finished with training loss 81353.10234375\n",
      "Epoch: 678 finished with training loss 81353.034375\n",
      "Epoch: 679 finished with training loss 81352.5453125\n",
      "Epoch: 680 finished with training loss 81352.6427734375\n",
      "Epoch: 681 finished with training loss 81351.8845703125\n",
      "Epoch: 682 finished with training loss 81351.79609375\n",
      "Epoch: 683 finished with training loss 81351.5560546875\n",
      "Epoch: 684 finished with training loss 81351.36953125\n",
      "Epoch: 685 finished with training loss 81351.2208984375\n",
      "Epoch: 686 finished with training loss 81350.9064453125\n",
      "Epoch: 687 finished with training loss 81350.5974609375\n",
      "Epoch: 688 finished with training loss 81350.4650390625\n",
      "Epoch: 689 finished with training loss 81350.067578125\n",
      "Epoch: 690 finished with training loss 81349.7591796875\n",
      "Epoch: 691 finished with training loss 81349.633203125\n",
      "Epoch: 692 finished with training loss 81349.45546875\n",
      "Epoch: 693 finished with training loss 81349.2509765625\n",
      "Epoch: 694 finished with training loss 81348.7427734375\n",
      "Epoch: 695 finished with training loss 81348.3458984375\n",
      "Epoch: 696 finished with training loss 81348.166015625\n",
      "Epoch: 697 finished with training loss 81347.6150390625\n",
      "Epoch: 698 finished with training loss 81347.596875\n",
      "Epoch: 699 finished with training loss 81347.2890625\n",
      "Epoch: 700 finished with training loss 81347.2212890625\n",
      "Epoch: 701 finished with training loss 81347.0287109375\n",
      "Epoch: 702 finished with training loss 81346.9173828125\n",
      "Epoch: 703 finished with training loss 81346.53125\n",
      "Epoch: 704 finished with training loss 81346.2052734375\n",
      "Epoch: 705 finished with training loss 81346.0185546875\n",
      "Epoch: 706 finished with training loss 81346.1494140625\n",
      "Epoch: 707 finished with training loss 81345.6671875\n",
      "Epoch: 708 finished with training loss 81345.5263671875\n",
      "Epoch: 709 finished with training loss 81345.29375\n",
      "Epoch: 710 finished with training loss 81344.9626953125\n",
      "Epoch: 711 finished with training loss 81344.56796875\n",
      "Epoch: 712 finished with training loss 81344.19140625\n",
      "Epoch: 713 finished with training loss 81344.0462890625\n",
      "Epoch: 714 finished with training loss 81343.63671875\n",
      "Epoch: 715 finished with training loss 81343.2533203125\n",
      "Epoch: 716 finished with training loss 81343.149609375\n",
      "Epoch: 717 finished with training loss 81342.8513671875\n",
      "Epoch: 718 finished with training loss 81342.51796875\n",
      "Epoch: 719 finished with training loss 81342.322265625\n",
      "Epoch: 720 finished with training loss 81341.9609375\n",
      "Epoch: 721 finished with training loss 81341.7150390625\n",
      "Epoch: 722 finished with training loss 81341.40234375\n",
      "Epoch: 723 finished with training loss 81341.1298828125\n",
      "Epoch: 724 finished with training loss 81340.88671875\n",
      "Epoch: 725 finished with training loss 81340.7935546875\n",
      "Epoch: 726 finished with training loss 81340.246484375\n",
      "Epoch: 727 finished with training loss 81339.812109375\n",
      "Epoch: 728 finished with training loss 81339.65625\n",
      "Epoch: 729 finished with training loss 81339.4486328125\n",
      "Epoch: 730 finished with training loss 81339.175390625\n",
      "Epoch: 731 finished with training loss 81338.9986328125\n",
      "Epoch: 732 finished with training loss 81338.677734375\n",
      "Epoch: 733 finished with training loss 81338.1458984375\n",
      "Epoch: 734 finished with training loss 81337.989453125\n",
      "Epoch: 735 finished with training loss 81337.6470703125\n",
      "Epoch: 736 finished with training loss 81337.1521484375\n",
      "Epoch: 737 finished with training loss 81336.87578125\n",
      "Epoch: 738 finished with training loss 81336.59609375\n",
      "Epoch: 739 finished with training loss 81336.614453125\n",
      "Epoch: 740 finished with training loss 81336.436328125\n",
      "Epoch: 741 finished with training loss 81335.877734375\n",
      "Epoch: 742 finished with training loss 81335.52734375\n",
      "Epoch: 743 finished with training loss 81335.31328125\n",
      "Epoch: 744 finished with training loss 81334.9978515625\n",
      "Epoch: 745 finished with training loss 81334.8103515625\n",
      "Epoch: 746 finished with training loss 81334.4798828125\n",
      "Epoch: 747 finished with training loss 81334.3013671875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 748 finished with training loss 81333.9080078125\n",
      "Epoch: 749 finished with training loss 81333.530859375\n",
      "Epoch: 750 finished with training loss 81333.2822265625\n",
      "Epoch: 751 finished with training loss 81332.916015625\n",
      "Epoch: 752 finished with training loss 81332.8796875\n",
      "Epoch: 753 finished with training loss 81332.96640625\n",
      "Epoch: 754 finished with training loss 81332.748828125\n",
      "Epoch: 755 finished with training loss 81332.771875\n",
      "Epoch: 756 finished with training loss 81332.5513671875\n",
      "Epoch: 757 finished with training loss 81332.080078125\n",
      "Epoch: 758 finished with training loss 81331.4904296875\n",
      "Epoch: 759 finished with training loss 81331.2076171875\n",
      "Epoch: 760 finished with training loss 81330.88203125\n",
      "Epoch: 761 finished with training loss 81330.6240234375\n",
      "Epoch: 762 finished with training loss 81330.315625\n",
      "Epoch: 763 finished with training loss 81329.9921875\n",
      "Epoch: 764 finished with training loss 81329.8052734375\n",
      "Epoch: 765 finished with training loss 81329.4634765625\n",
      "Epoch: 766 finished with training loss 81329.146875\n",
      "Epoch: 767 finished with training loss 81328.8294921875\n",
      "Epoch: 768 finished with training loss 81328.7603515625\n",
      "Epoch: 769 finished with training loss 81328.497265625\n",
      "Epoch: 770 finished with training loss 81328.2892578125\n",
      "Epoch: 771 finished with training loss 81328.1947265625\n",
      "Epoch: 772 finished with training loss 81327.960546875\n",
      "Epoch: 773 finished with training loss 81327.665234375\n",
      "Epoch: 774 finished with training loss 81327.459765625\n",
      "Epoch: 775 finished with training loss 81327.06328125\n",
      "Epoch: 776 finished with training loss 81326.747265625\n",
      "Epoch: 777 finished with training loss 81326.6447265625\n",
      "Epoch: 778 finished with training loss 81326.2572265625\n",
      "Epoch: 779 finished with training loss 81325.96015625\n",
      "Epoch: 780 finished with training loss 81325.786328125\n",
      "Epoch: 781 finished with training loss 81325.3857421875\n",
      "Epoch: 782 finished with training loss 81325.336328125\n",
      "Epoch: 783 finished with training loss 81325.22578125\n",
      "Epoch: 784 finished with training loss 81324.86171875\n",
      "Epoch: 785 finished with training loss 81324.5080078125\n",
      "Epoch: 786 finished with training loss 81324.2173828125\n",
      "Epoch: 787 finished with training loss 81324.05078125\n",
      "Epoch: 788 finished with training loss 81323.7595703125\n",
      "Epoch: 789 finished with training loss 81323.5822265625\n",
      "Epoch: 790 finished with training loss 81323.1017578125\n",
      "Epoch: 791 finished with training loss 81322.7125\n",
      "Epoch: 792 finished with training loss 81322.5927734375\n",
      "Epoch: 793 finished with training loss 81322.3421875\n",
      "Epoch: 794 finished with training loss 81321.930859375\n",
      "Epoch: 795 finished with training loss 81321.70390625\n",
      "Epoch: 796 finished with training loss 81321.312109375\n",
      "Epoch: 797 finished with training loss 81320.978125\n",
      "Epoch: 798 finished with training loss 81320.7662109375\n",
      "Epoch: 799 finished with training loss 81320.302734375\n",
      "Epoch: 800 finished with training loss 81319.9560546875\n",
      "Epoch: 801 finished with training loss 81319.578125\n",
      "Epoch: 802 finished with training loss 81319.6296875\n",
      "Epoch: 803 finished with training loss 81319.246875\n",
      "Epoch: 804 finished with training loss 81319.1109375\n",
      "Epoch: 805 finished with training loss 81318.9431640625\n",
      "Epoch: 806 finished with training loss 81318.6560546875\n",
      "Epoch: 807 finished with training loss 81318.3388671875\n",
      "Epoch: 808 finished with training loss 81318.0384765625\n",
      "Epoch: 809 finished with training loss 81317.69453125\n",
      "Epoch: 810 finished with training loss 81317.28046875\n",
      "Epoch: 811 finished with training loss 81316.83125\n",
      "Epoch: 812 finished with training loss 81316.83828125\n",
      "Epoch: 813 finished with training loss 81316.65546875\n",
      "Epoch: 814 finished with training loss 81316.0115234375\n",
      "Epoch: 815 finished with training loss 81315.908984375\n",
      "Epoch: 816 finished with training loss 81315.5744140625\n",
      "Epoch: 817 finished with training loss 81315.467578125\n",
      "Epoch: 818 finished with training loss 81315.117578125\n",
      "Epoch: 819 finished with training loss 81314.768359375\n",
      "Epoch: 820 finished with training loss 81314.608203125\n",
      "Epoch: 821 finished with training loss 81314.347265625\n",
      "Epoch: 822 finished with training loss 81314.0939453125\n",
      "Epoch: 823 finished with training loss 81313.46875\n",
      "Epoch: 824 finished with training loss 81312.8056640625\n",
      "Epoch: 825 finished with training loss 81312.629296875\n",
      "Epoch: 826 finished with training loss 81312.087109375\n",
      "Epoch: 827 finished with training loss 81311.9341796875\n",
      "Epoch: 828 finished with training loss 81311.365625\n",
      "Epoch: 829 finished with training loss 81311.11875\n",
      "Epoch: 830 finished with training loss 81310.8130859375\n",
      "Epoch: 831 finished with training loss 81310.6234375\n",
      "Epoch: 832 finished with training loss 81310.3142578125\n",
      "Epoch: 833 finished with training loss 81310.0544921875\n",
      "Epoch: 834 finished with training loss 81310.168359375\n",
      "Epoch: 835 finished with training loss 81309.4541015625\n",
      "Epoch: 836 finished with training loss 81309.46015625\n",
      "Epoch: 837 finished with training loss 81309.0982421875\n",
      "Epoch: 838 finished with training loss 81308.9802734375\n",
      "Epoch: 839 finished with training loss 81308.6427734375\n",
      "Epoch: 840 finished with training loss 81308.4548828125\n",
      "Epoch: 841 finished with training loss 81307.8896484375\n",
      "Epoch: 842 finished with training loss 81307.3375\n",
      "Epoch: 843 finished with training loss 81307.1630859375\n",
      "Epoch: 844 finished with training loss 81306.9787109375\n",
      "Epoch: 845 finished with training loss 81306.8580078125\n",
      "Epoch: 846 finished with training loss 81306.5318359375\n",
      "Epoch: 847 finished with training loss 81305.459375\n",
      "Epoch: 848 finished with training loss 81305.2998046875\n",
      "Epoch: 849 finished with training loss 81304.826953125\n",
      "Epoch: 850 finished with training loss 81304.598046875\n",
      "Epoch: 851 finished with training loss 81304.6365234375\n",
      "Epoch: 852 finished with training loss 81303.8013671875\n",
      "Epoch: 853 finished with training loss 81303.678515625\n",
      "Epoch: 854 finished with training loss 81303.5759765625\n",
      "Epoch: 855 finished with training loss 81303.4611328125\n",
      "Epoch: 856 finished with training loss 81303.11171875\n",
      "Epoch: 857 finished with training loss 81302.9439453125\n",
      "Epoch: 858 finished with training loss 81302.75546875\n",
      "Epoch: 859 finished with training loss 81302.360546875\n",
      "Epoch: 860 finished with training loss 81302.27890625\n",
      "Epoch: 861 finished with training loss 81301.7955078125\n",
      "Epoch: 862 finished with training loss 81301.6501953125\n",
      "Epoch: 863 finished with training loss 81301.2466796875\n",
      "Epoch: 864 finished with training loss 81301.083203125\n",
      "Epoch: 865 finished with training loss 81300.785546875\n",
      "Epoch: 866 finished with training loss 81300.3953125\n",
      "Epoch: 867 finished with training loss 81300.018359375\n",
      "Epoch: 868 finished with training loss 81299.57734375\n",
      "Epoch: 869 finished with training loss 81299.3525390625\n",
      "Epoch: 870 finished with training loss 81298.79765625\n",
      "Epoch: 871 finished with training loss 81297.9306640625\n",
      "Epoch: 872 finished with training loss 81297.0873046875\n",
      "Epoch: 873 finished with training loss 81296.8869140625\n",
      "Epoch: 874 finished with training loss 81296.35234375\n",
      "Epoch: 875 finished with training loss 81295.880078125\n",
      "Epoch: 876 finished with training loss 81295.543359375\n",
      "Epoch: 877 finished with training loss 81295.4580078125\n",
      "Epoch: 878 finished with training loss 81295.12265625\n",
      "Epoch: 879 finished with training loss 81294.8927734375\n",
      "Epoch: 880 finished with training loss 81294.8599609375\n",
      "Epoch: 881 finished with training loss 81294.7125\n",
      "Epoch: 882 finished with training loss 81294.3841796875\n",
      "Epoch: 883 finished with training loss 81294.0130859375\n",
      "Epoch: 884 finished with training loss 81293.319140625\n",
      "Epoch: 885 finished with training loss 81292.9390625\n",
      "Epoch: 886 finished with training loss 81292.6875\n",
      "Epoch: 887 finished with training loss 81292.3333984375\n",
      "Epoch: 888 finished with training loss 81291.983203125\n",
      "Epoch: 889 finished with training loss 81291.8072265625\n",
      "Epoch: 890 finished with training loss 81291.5564453125\n",
      "Epoch: 891 finished with training loss 81291.3513671875\n",
      "Epoch: 892 finished with training loss 81291.1759765625\n",
      "Epoch: 893 finished with training loss 81290.8279296875\n",
      "Epoch: 894 finished with training loss 81290.2025390625\n",
      "Epoch: 895 finished with training loss 81289.8552734375\n",
      "Epoch: 896 finished with training loss 81289.7759765625\n",
      "Epoch: 897 finished with training loss 81289.41640625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 898 finished with training loss 81289.1564453125\n",
      "Epoch: 899 finished with training loss 81288.8626953125\n",
      "Epoch: 900 finished with training loss 81288.6060546875\n",
      "Epoch: 901 finished with training loss 81288.25\n",
      "Epoch: 902 finished with training loss 81287.82109375\n",
      "Epoch: 903 finished with training loss 81287.6591796875\n",
      "Epoch: 904 finished with training loss 81287.4779296875\n",
      "Epoch: 905 finished with training loss 81287.339453125\n",
      "Epoch: 906 finished with training loss 81286.946875\n",
      "Epoch: 907 finished with training loss 81286.8419921875\n",
      "Epoch: 908 finished with training loss 81286.92734375\n",
      "Epoch: 909 finished with training loss 81286.4345703125\n",
      "Epoch: 910 finished with training loss 81286.0412109375\n",
      "Epoch: 911 finished with training loss 81285.2498046875\n",
      "Epoch: 912 finished with training loss 81284.8685546875\n",
      "Epoch: 913 finished with training loss 81284.2970703125\n",
      "Epoch: 914 finished with training loss 81284.1798828125\n",
      "Epoch: 915 finished with training loss 81283.7392578125\n",
      "Epoch: 916 finished with training loss 81283.506640625\n",
      "Epoch: 917 finished with training loss 81283.2525390625\n",
      "Epoch: 918 finished with training loss 81283.0138671875\n",
      "Epoch: 919 finished with training loss 81282.5212890625\n",
      "Epoch: 920 finished with training loss 81282.28359375\n",
      "Epoch: 921 finished with training loss 81281.9087890625\n",
      "Epoch: 922 finished with training loss 81281.7791015625\n",
      "Epoch: 923 finished with training loss 81281.4703125\n",
      "Epoch: 924 finished with training loss 81281.2720703125\n",
      "Epoch: 925 finished with training loss 81281.142578125\n",
      "Epoch: 926 finished with training loss 81280.7126953125\n",
      "Epoch: 927 finished with training loss 81280.7642578125\n",
      "Epoch: 928 finished with training loss 81280.1779296875\n",
      "Epoch: 929 finished with training loss 81279.91484375\n",
      "Epoch: 930 finished with training loss 81279.5654296875\n",
      "Epoch: 931 finished with training loss 81279.2544921875\n",
      "Epoch: 932 finished with training loss 81278.888671875\n",
      "Epoch: 933 finished with training loss 81278.48046875\n",
      "Epoch: 934 finished with training loss 81278.408984375\n",
      "Epoch: 935 finished with training loss 81278.0828125\n",
      "Epoch: 936 finished with training loss 81278.0404296875\n",
      "Epoch: 937 finished with training loss 81277.6849609375\n",
      "Epoch: 938 finished with training loss 81277.316796875\n",
      "Epoch: 939 finished with training loss 81277.0482421875\n",
      "Epoch: 940 finished with training loss 81276.7091796875\n",
      "Epoch: 941 finished with training loss 81276.437109375\n",
      "Epoch: 942 finished with training loss 81275.93203125\n",
      "Epoch: 943 finished with training loss 81275.8513671875\n",
      "Epoch: 944 finished with training loss 81275.7515625\n",
      "Epoch: 945 finished with training loss 81275.3279296875\n",
      "Epoch: 946 finished with training loss 81274.9271484375\n",
      "Epoch: 947 finished with training loss 81274.6576171875\n",
      "Epoch: 948 finished with training loss 81274.141796875\n",
      "Epoch: 949 finished with training loss 81274.0310546875\n",
      "Epoch: 950 finished with training loss 81273.902734375\n",
      "Epoch: 951 finished with training loss 81273.625390625\n",
      "Epoch: 952 finished with training loss 81273.1369140625\n",
      "Epoch: 953 finished with training loss 81272.9740234375\n",
      "Epoch: 954 finished with training loss 81272.4431640625\n",
      "Epoch: 955 finished with training loss 81272.2541015625\n",
      "Epoch: 956 finished with training loss 81272.165234375\n",
      "Epoch: 957 finished with training loss 81271.8708984375\n",
      "Epoch: 958 finished with training loss 81271.5876953125\n",
      "Epoch: 959 finished with training loss 81271.466796875\n",
      "Epoch: 960 finished with training loss 81271.259375\n",
      "Epoch: 961 finished with training loss 81271.1716796875\n",
      "Epoch: 962 finished with training loss 81270.6248046875\n",
      "Epoch: 963 finished with training loss 81270.332421875\n",
      "Epoch: 964 finished with training loss 81270.1599609375\n",
      "Epoch: 965 finished with training loss 81270.04765625\n",
      "Epoch: 966 finished with training loss 81269.7548828125\n",
      "Epoch: 967 finished with training loss 81269.4423828125\n",
      "Epoch: 968 finished with training loss 81268.7599609375\n",
      "Epoch: 969 finished with training loss 81268.6939453125\n",
      "Epoch: 970 finished with training loss 81268.4689453125\n",
      "Epoch: 971 finished with training loss 81268.266796875\n",
      "Epoch: 972 finished with training loss 81268.1009765625\n",
      "Epoch: 973 finished with training loss 81267.46953125\n",
      "Epoch: 974 finished with training loss 81267.2453125\n",
      "Epoch: 975 finished with training loss 81267.17578125\n",
      "Epoch: 976 finished with training loss 81266.8470703125\n",
      "Epoch: 977 finished with training loss 81266.576171875\n",
      "Epoch: 978 finished with training loss 81266.2498046875\n",
      "Epoch: 979 finished with training loss 81265.8640625\n",
      "Epoch: 980 finished with training loss 81265.8625\n",
      "Epoch: 981 finished with training loss 81265.640625\n",
      "Epoch: 982 finished with training loss 81265.1943359375\n",
      "Epoch: 983 finished with training loss 81265.058203125\n",
      "Epoch: 984 finished with training loss 81264.6244140625\n",
      "Epoch: 985 finished with training loss 81264.02578125\n",
      "Epoch: 986 finished with training loss 81263.8658203125\n",
      "Epoch: 987 finished with training loss 81263.621875\n",
      "Epoch: 988 finished with training loss 81263.0513671875\n",
      "Epoch: 989 finished with training loss 81262.903125\n",
      "Epoch: 990 finished with training loss 81262.7380859375\n",
      "Epoch: 991 finished with training loss 81262.4892578125\n",
      "Epoch: 992 finished with training loss 81262.2302734375\n",
      "Epoch: 993 finished with training loss 81261.9462890625\n",
      "Epoch: 994 finished with training loss 81261.6611328125\n",
      "Epoch: 995 finished with training loss 81261.3091796875\n",
      "Epoch: 996 finished with training loss 81261.193359375\n",
      "Epoch: 997 finished with training loss 81260.821875\n",
      "Epoch: 998 finished with training loss 81260.372265625\n",
      "Epoch: 999 finished with training loss 81260.23515625\n",
      "Epoch: 1000 finished with training loss 81259.959375\n",
      "Epoch: 1001 finished with training loss 81259.7017578125\n",
      "Epoch: 1002 finished with training loss 81259.19140625\n",
      "Epoch: 1003 finished with training loss 81259.132421875\n",
      "Epoch: 1004 finished with training loss 81258.8689453125\n",
      "Epoch: 1005 finished with training loss 81258.54765625\n",
      "Epoch: 1006 finished with training loss 81258.356640625\n",
      "Epoch: 1007 finished with training loss 81258.0384765625\n",
      "Epoch: 1008 finished with training loss 81257.5080078125\n",
      "Epoch: 1009 finished with training loss 81257.367578125\n",
      "Epoch: 1010 finished with training loss 81257.09609375\n",
      "Epoch: 1011 finished with training loss 81256.81171875\n",
      "Epoch: 1012 finished with training loss 81256.7115234375\n",
      "Epoch: 1013 finished with training loss 81256.3345703125\n",
      "Epoch: 1014 finished with training loss 81255.98046875\n",
      "Epoch: 1015 finished with training loss 81255.49296875\n",
      "Epoch: 1016 finished with training loss 81255.37578125\n",
      "Epoch: 1017 finished with training loss 81255.17578125\n",
      "Epoch: 1018 finished with training loss 81254.95625\n",
      "Epoch: 1019 finished with training loss 81254.6150390625\n",
      "Epoch: 1020 finished with training loss 81254.4302734375\n",
      "Epoch: 1021 finished with training loss 81254.062109375\n",
      "Epoch: 1022 finished with training loss 81253.8541015625\n",
      "Epoch: 1023 finished with training loss 81253.350390625\n",
      "Epoch: 1024 finished with training loss 81253.3029296875\n",
      "Epoch: 1025 finished with training loss 81253.0998046875\n",
      "Epoch: 1026 finished with training loss 81252.964453125\n",
      "Epoch: 1027 finished with training loss 81252.812890625\n",
      "Epoch: 1028 finished with training loss 81252.1232421875\n",
      "Epoch: 1029 finished with training loss 81251.776953125\n",
      "Epoch: 1030 finished with training loss 81251.5384765625\n",
      "Epoch: 1031 finished with training loss 81251.46640625\n",
      "Epoch: 1032 finished with training loss 81251.0400390625\n",
      "Epoch: 1033 finished with training loss 81250.636328125\n",
      "Epoch: 1034 finished with training loss 81250.2865234375\n",
      "Epoch: 1035 finished with training loss 81250.049609375\n",
      "Epoch: 1036 finished with training loss 81249.741015625\n",
      "Epoch: 1037 finished with training loss 81249.2458984375\n",
      "Epoch: 1038 finished with training loss 81249.2236328125\n",
      "Epoch: 1039 finished with training loss 81249.015625\n",
      "Epoch: 1040 finished with training loss 81248.76484375\n",
      "Epoch: 1041 finished with training loss 81248.1095703125\n",
      "Epoch: 1042 finished with training loss 81247.9259765625\n",
      "Epoch: 1043 finished with training loss 81247.7388671875\n",
      "Epoch: 1044 finished with training loss 81247.6005859375\n",
      "Epoch: 1045 finished with training loss 81247.198046875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1046 finished with training loss 81247.220703125\n",
      "Epoch: 1047 finished with training loss 81246.8505859375\n",
      "Epoch: 1048 finished with training loss 81246.1396484375\n",
      "Epoch: 1049 finished with training loss 81245.5814453125\n",
      "Epoch: 1050 finished with training loss 81244.96875\n",
      "Epoch: 1051 finished with training loss 81244.251171875\n",
      "Epoch: 1052 finished with training loss 81243.744140625\n",
      "Epoch: 1053 finished with training loss 81243.0357421875\n",
      "Epoch: 1054 finished with training loss 81242.26015625\n",
      "Epoch: 1055 finished with training loss 81241.9404296875\n",
      "Epoch: 1056 finished with training loss 81241.6912109375\n",
      "Epoch: 1057 finished with training loss 81241.28359375\n",
      "Epoch: 1058 finished with training loss 81240.61796875\n",
      "Epoch: 1059 finished with training loss 81240.205078125\n",
      "Epoch: 1060 finished with training loss 81239.5146484375\n",
      "Epoch: 1061 finished with training loss 81239.3515625\n",
      "Epoch: 1062 finished with training loss 81238.99296875\n",
      "Epoch: 1063 finished with training loss 81238.6841796875\n",
      "Epoch: 1064 finished with training loss 81238.505859375\n",
      "Epoch: 1065 finished with training loss 81238.36484375\n",
      "Epoch: 1066 finished with training loss 81238.4126953125\n",
      "Epoch: 1067 finished with training loss 81237.24765625\n",
      "Epoch: 1068 finished with training loss 81237.234765625\n",
      "Epoch: 1069 finished with training loss 81237.038671875\n",
      "Epoch: 1070 finished with training loss 81236.3126953125\n",
      "Epoch: 1071 finished with training loss 81235.851171875\n",
      "Epoch: 1072 finished with training loss 81235.71484375\n",
      "Epoch: 1073 finished with training loss 81235.3732421875\n",
      "Epoch: 1074 finished with training loss 81235.0083984375\n",
      "Epoch: 1075 finished with training loss 81234.540234375\n",
      "Epoch: 1076 finished with training loss 81234.198046875\n",
      "Epoch: 1077 finished with training loss 81233.7068359375\n",
      "Epoch: 1078 finished with training loss 81233.4033203125\n",
      "Epoch: 1079 finished with training loss 81233.093359375\n",
      "Epoch: 1080 finished with training loss 81232.801171875\n",
      "Epoch: 1081 finished with training loss 81232.4638671875\n",
      "Epoch: 1082 finished with training loss 81232.166015625\n",
      "Epoch: 1083 finished with training loss 81231.821484375\n",
      "Epoch: 1084 finished with training loss 81231.408203125\n",
      "Epoch: 1085 finished with training loss 81231.0552734375\n",
      "Epoch: 1086 finished with training loss 81230.9771484375\n",
      "Epoch: 1087 finished with training loss 81230.3712890625\n",
      "Epoch: 1088 finished with training loss 81230.2998046875\n",
      "Epoch: 1089 finished with training loss 81229.7943359375\n",
      "Epoch: 1090 finished with training loss 81229.7671875\n",
      "Epoch: 1091 finished with training loss 81229.3810546875\n",
      "Epoch: 1092 finished with training loss 81228.858203125\n",
      "Epoch: 1093 finished with training loss 81228.75703125\n",
      "Epoch: 1094 finished with training loss 81228.332421875\n",
      "Epoch: 1095 finished with training loss 81228.1689453125\n",
      "Epoch: 1096 finished with training loss 81227.64453125\n",
      "Epoch: 1097 finished with training loss 81227.2283203125\n",
      "Epoch: 1098 finished with training loss 81226.9095703125\n",
      "Epoch: 1099 finished with training loss 81226.6662109375\n",
      "Epoch: 1100 finished with training loss 81226.53359375\n",
      "Epoch: 1101 finished with training loss 81226.2427734375\n",
      "Epoch: 1102 finished with training loss 81225.9853515625\n",
      "Epoch: 1103 finished with training loss 81225.71640625\n",
      "Epoch: 1104 finished with training loss 81225.1974609375\n",
      "Epoch: 1105 finished with training loss 81224.9162109375\n",
      "Epoch: 1106 finished with training loss 81224.6412109375\n",
      "Epoch: 1107 finished with training loss 81224.0341796875\n",
      "Epoch: 1108 finished with training loss 81224.1294921875\n",
      "Epoch: 1109 finished with training loss 81223.8283203125\n",
      "Epoch: 1110 finished with training loss 81223.341015625\n",
      "Epoch: 1111 finished with training loss 81223.223046875\n",
      "Epoch: 1112 finished with training loss 81222.8380859375\n",
      "Epoch: 1113 finished with training loss 81222.2287109375\n",
      "Epoch: 1114 finished with training loss 81221.7328125\n",
      "Epoch: 1115 finished with training loss 81221.57890625\n",
      "Epoch: 1116 finished with training loss 81221.2392578125\n",
      "Epoch: 1117 finished with training loss 81221.042578125\n",
      "Epoch: 1118 finished with training loss 81220.59296875\n",
      "Epoch: 1119 finished with training loss 81220.2244140625\n",
      "Epoch: 1120 finished with training loss 81219.836328125\n",
      "Epoch: 1121 finished with training loss 81219.442578125\n",
      "Epoch: 1122 finished with training loss 81218.9970703125\n",
      "Epoch: 1123 finished with training loss 81218.572265625\n",
      "Epoch: 1124 finished with training loss 81218.163671875\n",
      "Epoch: 1125 finished with training loss 81217.7283203125\n",
      "Epoch: 1126 finished with training loss 81217.5798828125\n",
      "Epoch: 1127 finished with training loss 81217.078515625\n",
      "Epoch: 1128 finished with training loss 81216.757421875\n",
      "Epoch: 1129 finished with training loss 81216.3685546875\n",
      "Epoch: 1130 finished with training loss 81215.8609375\n",
      "Epoch: 1131 finished with training loss 81215.3962890625\n",
      "Epoch: 1132 finished with training loss 81215.223828125\n",
      "Epoch: 1133 finished with training loss 81214.84921875\n",
      "Epoch: 1134 finished with training loss 81214.6361328125\n",
      "Epoch: 1135 finished with training loss 81214.31796875\n",
      "Epoch: 1136 finished with training loss 81213.808984375\n",
      "Epoch: 1137 finished with training loss 81213.6806640625\n",
      "Epoch: 1138 finished with training loss 81213.2873046875\n",
      "Epoch: 1139 finished with training loss 81212.8349609375\n",
      "Epoch: 1140 finished with training loss 81212.4751953125\n",
      "Epoch: 1141 finished with training loss 81212.3421875\n",
      "Epoch: 1142 finished with training loss 81212.0736328125\n",
      "Epoch: 1143 finished with training loss 81211.691796875\n",
      "Epoch: 1144 finished with training loss 81211.1693359375\n",
      "Epoch: 1145 finished with training loss 81210.9857421875\n",
      "Epoch: 1146 finished with training loss 81210.7896484375\n",
      "Epoch: 1147 finished with training loss 81210.5578125\n",
      "Epoch: 1148 finished with training loss 81210.4603515625\n",
      "Epoch: 1149 finished with training loss 81209.99609375\n",
      "Epoch: 1150 finished with training loss 81209.9048828125\n",
      "Epoch: 1151 finished with training loss 81209.56953125\n",
      "Epoch: 1152 finished with training loss 81209.0166015625\n",
      "Epoch: 1153 finished with training loss 81208.9552734375\n",
      "Epoch: 1154 finished with training loss 81208.368359375\n",
      "Epoch: 1155 finished with training loss 81208.1154296875\n",
      "Epoch: 1156 finished with training loss 81207.9337890625\n",
      "Epoch: 1157 finished with training loss 81207.8064453125\n",
      "Epoch: 1158 finished with training loss 81207.3669921875\n",
      "Epoch: 1159 finished with training loss 81206.9037109375\n",
      "Epoch: 1160 finished with training loss 81206.4484375\n",
      "Epoch: 1161 finished with training loss 81206.1255859375\n",
      "Epoch: 1162 finished with training loss 81205.68671875\n",
      "Epoch: 1163 finished with training loss 81205.5943359375\n",
      "Epoch: 1164 finished with training loss 81205.4912109375\n",
      "Epoch: 1165 finished with training loss 81204.9171875\n",
      "Epoch: 1166 finished with training loss 81204.7138671875\n",
      "Epoch: 1167 finished with training loss 81204.522265625\n",
      "Epoch: 1168 finished with training loss 81204.4484375\n",
      "Epoch: 1169 finished with training loss 81203.8958984375\n",
      "Epoch: 1170 finished with training loss 81203.3796875\n",
      "Epoch: 1171 finished with training loss 81203.3197265625\n",
      "Epoch: 1172 finished with training loss 81203.0423828125\n",
      "Epoch: 1173 finished with training loss 81202.802734375\n",
      "Epoch: 1174 finished with training loss 81202.302734375\n",
      "Epoch: 1175 finished with training loss 81201.980078125\n",
      "Epoch: 1176 finished with training loss 81201.70234375\n",
      "Epoch: 1177 finished with training loss 81201.6060546875\n",
      "Epoch: 1178 finished with training loss 81201.0724609375\n",
      "Epoch: 1179 finished with training loss 81200.928515625\n",
      "Epoch: 1180 finished with training loss 81200.4224609375\n",
      "Epoch: 1181 finished with training loss 81200.25234375\n",
      "Epoch: 1182 finished with training loss 81200.0990234375\n",
      "Epoch: 1183 finished with training loss 81199.665625\n",
      "Epoch: 1184 finished with training loss 81199.40390625\n",
      "Epoch: 1185 finished with training loss 81199.10078125\n",
      "Epoch: 1186 finished with training loss 81198.72890625\n",
      "Epoch: 1187 finished with training loss 81198.5283203125\n",
      "Epoch: 1188 finished with training loss 81198.5068359375\n",
      "Epoch: 1189 finished with training loss 81198.2662109375\n",
      "Epoch: 1190 finished with training loss 81198.1146484375\n",
      "Epoch: 1191 finished with training loss 81197.8291015625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1192 finished with training loss 81197.49765625\n",
      "Epoch: 1193 finished with training loss 81196.7578125\n",
      "Epoch: 1194 finished with training loss 81196.494921875\n",
      "Epoch: 1195 finished with training loss 81195.840625\n",
      "Epoch: 1196 finished with training loss 81195.7853515625\n",
      "Epoch: 1197 finished with training loss 81195.5734375\n",
      "Epoch: 1198 finished with training loss 81195.4244140625\n",
      "Epoch: 1199 finished with training loss 81195.057421875\n",
      "Epoch: 1200 finished with training loss 81194.778125\n",
      "Epoch: 1201 finished with training loss 81194.413671875\n",
      "Epoch: 1202 finished with training loss 81194.151953125\n",
      "Epoch: 1203 finished with training loss 81193.9439453125\n",
      "Epoch: 1204 finished with training loss 81193.8599609375\n",
      "Epoch: 1205 finished with training loss 81193.5841796875\n",
      "Epoch: 1206 finished with training loss 81193.325390625\n",
      "Epoch: 1207 finished with training loss 81192.8236328125\n",
      "Epoch: 1208 finished with training loss 81192.4162109375\n",
      "Epoch: 1209 finished with training loss 81192.07890625\n",
      "Epoch: 1210 finished with training loss 81191.826953125\n",
      "Epoch: 1211 finished with training loss 81191.5421875\n",
      "Epoch: 1212 finished with training loss 81191.2560546875\n",
      "Epoch: 1213 finished with training loss 81190.875\n",
      "Epoch: 1214 finished with training loss 81190.5171875\n",
      "Epoch: 1215 finished with training loss 81190.1158203125\n",
      "Epoch: 1216 finished with training loss 81190.1076171875\n",
      "Epoch: 1217 finished with training loss 81189.8537109375\n",
      "Epoch: 1218 finished with training loss 81189.29296875\n",
      "Epoch: 1219 finished with training loss 81189.2029296875\n",
      "Epoch: 1220 finished with training loss 81188.687109375\n",
      "Epoch: 1221 finished with training loss 81188.2951171875\n",
      "Epoch: 1222 finished with training loss 81188.21328125\n",
      "Epoch: 1223 finished with training loss 81187.9240234375\n",
      "Epoch: 1224 finished with training loss 81187.4328125\n",
      "Epoch: 1225 finished with training loss 81186.767578125\n",
      "Epoch: 1226 finished with training loss 81186.5828125\n",
      "Epoch: 1227 finished with training loss 81186.3216796875\n",
      "Epoch: 1228 finished with training loss 81186.18984375\n",
      "Epoch: 1229 finished with training loss 81185.88671875\n",
      "Epoch: 1230 finished with training loss 81185.78828125\n",
      "Epoch: 1231 finished with training loss 81185.3880859375\n",
      "Epoch: 1232 finished with training loss 81185.1962890625\n",
      "Epoch: 1233 finished with training loss 81184.730078125\n",
      "Epoch: 1234 finished with training loss 81184.3978515625\n",
      "Epoch: 1235 finished with training loss 81183.901171875\n",
      "Epoch: 1236 finished with training loss 81183.569140625\n",
      "Epoch: 1237 finished with training loss 81183.1990234375\n",
      "Epoch: 1238 finished with training loss 81182.727734375\n",
      "Epoch: 1239 finished with training loss 81182.6984375\n",
      "Epoch: 1240 finished with training loss 81182.3150390625\n",
      "Epoch: 1241 finished with training loss 81182.041015625\n",
      "Epoch: 1242 finished with training loss 81181.6326171875\n",
      "Epoch: 1243 finished with training loss 81181.3318359375\n",
      "Epoch: 1244 finished with training loss 81180.996875\n",
      "Epoch: 1245 finished with training loss 81180.8041015625\n",
      "Epoch: 1246 finished with training loss 81180.2166015625\n",
      "Epoch: 1247 finished with training loss 81180.4658203125\n",
      "Epoch: 1248 finished with training loss 81180.15703125\n",
      "Epoch: 1249 finished with training loss 81179.6771484375\n",
      "Epoch: 1250 finished with training loss 81179.4833984375\n",
      "Epoch: 1251 finished with training loss 81179.260546875\n",
      "Epoch: 1252 finished with training loss 81178.9900390625\n",
      "Epoch: 1253 finished with training loss 81178.575390625\n",
      "Epoch: 1254 finished with training loss 81178.3861328125\n",
      "Epoch: 1255 finished with training loss 81178.0642578125\n",
      "Epoch: 1256 finished with training loss 81177.683984375\n",
      "Epoch: 1257 finished with training loss 81177.084765625\n",
      "Epoch: 1258 finished with training loss 81176.6837890625\n",
      "Epoch: 1259 finished with training loss 81176.11328125\n",
      "Epoch: 1260 finished with training loss 81176.04375\n",
      "Epoch: 1261 finished with training loss 81175.64296875\n",
      "Epoch: 1262 finished with training loss 81175.4388671875\n",
      "Epoch: 1263 finished with training loss 81175.2546875\n",
      "Epoch: 1264 finished with training loss 81174.7958984375\n",
      "Epoch: 1265 finished with training loss 81174.6490234375\n",
      "Epoch: 1266 finished with training loss 81174.3556640625\n",
      "Epoch: 1267 finished with training loss 81174.1337890625\n",
      "Epoch: 1268 finished with training loss 81173.8841796875\n",
      "Epoch: 1269 finished with training loss 81173.540625\n",
      "Epoch: 1270 finished with training loss 81173.2541015625\n",
      "Epoch: 1271 finished with training loss 81173.024609375\n",
      "Epoch: 1272 finished with training loss 81172.4708984375\n",
      "Epoch: 1273 finished with training loss 81172.0529296875\n",
      "Epoch: 1274 finished with training loss 81171.5451171875\n",
      "Epoch: 1275 finished with training loss 81171.3060546875\n",
      "Epoch: 1276 finished with training loss 81171.308984375\n",
      "Epoch: 1277 finished with training loss 81171.0884765625\n",
      "Epoch: 1278 finished with training loss 81170.6365234375\n",
      "Epoch: 1279 finished with training loss 81170.1759765625\n",
      "Epoch: 1280 finished with training loss 81169.8494140625\n",
      "Epoch: 1281 finished with training loss 81169.320703125\n",
      "Epoch: 1282 finished with training loss 81169.1978515625\n",
      "Epoch: 1283 finished with training loss 81168.915234375\n",
      "Epoch: 1284 finished with training loss 81168.5650390625\n",
      "Epoch: 1285 finished with training loss 81168.1162109375\n",
      "Epoch: 1286 finished with training loss 81167.9888671875\n",
      "Epoch: 1287 finished with training loss 81167.7546875\n",
      "Epoch: 1288 finished with training loss 81167.5814453125\n",
      "Epoch: 1289 finished with training loss 81167.2166015625\n",
      "Epoch: 1290 finished with training loss 81167.2560546875\n",
      "Epoch: 1291 finished with training loss 81166.9138671875\n",
      "Epoch: 1292 finished with training loss 81166.8041015625\n",
      "Epoch: 1293 finished with training loss 81166.5291015625\n",
      "Epoch: 1294 finished with training loss 81166.3275390625\n",
      "Epoch: 1295 finished with training loss 81165.9408203125\n",
      "Epoch: 1296 finished with training loss 81165.686328125\n",
      "Epoch: 1297 finished with training loss 81165.23359375\n",
      "Epoch: 1298 finished with training loss 81164.878125\n",
      "Epoch: 1299 finished with training loss 81164.60703125\n",
      "Epoch: 1300 finished with training loss 81164.4666015625\n",
      "Epoch: 1301 finished with training loss 81163.7732421875\n",
      "Epoch: 1302 finished with training loss 81163.700390625\n",
      "Epoch: 1303 finished with training loss 81162.9439453125\n",
      "Epoch: 1304 finished with training loss 81162.669921875\n",
      "Epoch: 1305 finished with training loss 81162.4025390625\n",
      "Epoch: 1306 finished with training loss 81162.0412109375\n",
      "Epoch: 1307 finished with training loss 81161.508203125\n",
      "Epoch: 1308 finished with training loss 81161.171875\n",
      "Epoch: 1309 finished with training loss 81160.8521484375\n",
      "Epoch: 1310 finished with training loss 81160.6599609375\n",
      "Epoch: 1311 finished with training loss 81160.1630859375\n",
      "Epoch: 1312 finished with training loss 81160.2107421875\n",
      "Epoch: 1313 finished with training loss 81159.8638671875\n",
      "Epoch: 1314 finished with training loss 81159.4900390625\n",
      "Epoch: 1315 finished with training loss 81158.9296875\n",
      "Epoch: 1316 finished with training loss 81158.6833984375\n",
      "Epoch: 1317 finished with training loss 81158.2384765625\n",
      "Epoch: 1318 finished with training loss 81158.259375\n",
      "Epoch: 1319 finished with training loss 81157.7287109375\n",
      "Epoch: 1320 finished with training loss 81157.546875\n",
      "Epoch: 1321 finished with training loss 81157.4220703125\n",
      "Epoch: 1322 finished with training loss 81156.8169921875\n",
      "Epoch: 1323 finished with training loss 81156.204296875\n",
      "Epoch: 1324 finished with training loss 81156.1453125\n",
      "Epoch: 1325 finished with training loss 81155.95546875\n",
      "Epoch: 1326 finished with training loss 81155.8453125\n",
      "Epoch: 1327 finished with training loss 81155.3015625\n",
      "Epoch: 1328 finished with training loss 81154.6544921875\n",
      "Epoch: 1329 finished with training loss 81153.92578125\n",
      "Epoch: 1330 finished with training loss 81153.5814453125\n",
      "Epoch: 1331 finished with training loss 81153.1232421875\n",
      "Epoch: 1332 finished with training loss 81152.7875\n",
      "Epoch: 1333 finished with training loss 81152.41015625\n",
      "Epoch: 1334 finished with training loss 81152.0279296875\n",
      "Epoch: 1335 finished with training loss 81151.6228515625\n",
      "Epoch: 1336 finished with training loss 81151.45390625\n",
      "Epoch: 1337 finished with training loss 81150.976953125\n",
      "Epoch: 1338 finished with training loss 81150.8326171875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1339 finished with training loss 81150.595703125\n",
      "Epoch: 1340 finished with training loss 81150.1150390625\n",
      "Epoch: 1341 finished with training loss 81150.0896484375\n",
      "Epoch: 1342 finished with training loss 81149.6279296875\n",
      "Epoch: 1343 finished with training loss 81149.15859375\n",
      "Epoch: 1344 finished with training loss 81148.931640625\n",
      "Epoch: 1345 finished with training loss 81148.6234375\n",
      "Epoch: 1346 finished with training loss 81148.30390625\n",
      "Epoch: 1347 finished with training loss 81148.2666015625\n",
      "Epoch: 1348 finished with training loss 81147.604296875\n",
      "Epoch: 1349 finished with training loss 81147.47265625\n",
      "Epoch: 1350 finished with training loss 81147.321875\n",
      "Epoch: 1351 finished with training loss 81146.987890625\n",
      "Epoch: 1352 finished with training loss 81146.480078125\n",
      "Epoch: 1353 finished with training loss 81146.123046875\n",
      "Epoch: 1354 finished with training loss 81145.556640625\n",
      "Epoch: 1355 finished with training loss 81145.148046875\n",
      "Epoch: 1356 finished with training loss 81144.791796875\n",
      "Epoch: 1357 finished with training loss 81144.5587890625\n",
      "Epoch: 1358 finished with training loss 81144.490625\n",
      "Epoch: 1359 finished with training loss 81144.4068359375\n",
      "Epoch: 1360 finished with training loss 81144.0154296875\n",
      "Epoch: 1361 finished with training loss 81143.7046875\n",
      "Epoch: 1362 finished with training loss 81143.221875\n",
      "Epoch: 1363 finished with training loss 81142.8873046875\n",
      "Epoch: 1364 finished with training loss 81142.585546875\n",
      "Epoch: 1365 finished with training loss 81141.998046875\n",
      "Epoch: 1366 finished with training loss 81141.9767578125\n",
      "Epoch: 1367 finished with training loss 81141.5033203125\n",
      "Epoch: 1368 finished with training loss 81141.3470703125\n",
      "Epoch: 1369 finished with training loss 81140.83046875\n",
      "Epoch: 1370 finished with training loss 81140.7712890625\n",
      "Epoch: 1371 finished with training loss 81140.29140625\n",
      "Epoch: 1372 finished with training loss 81139.9109375\n",
      "Epoch: 1373 finished with training loss 81139.7400390625\n",
      "Epoch: 1374 finished with training loss 81138.944140625\n",
      "Epoch: 1375 finished with training loss 81138.932421875\n",
      "Epoch: 1376 finished with training loss 81138.3169921875\n",
      "Epoch: 1377 finished with training loss 81137.701171875\n",
      "Epoch: 1378 finished with training loss 81137.6234375\n",
      "Epoch: 1379 finished with training loss 81137.369140625\n",
      "Epoch: 1380 finished with training loss 81137.083984375\n",
      "Epoch: 1381 finished with training loss 81136.4494140625\n",
      "Epoch: 1382 finished with training loss 81136.4833984375\n",
      "Epoch: 1383 finished with training loss 81136.3443359375\n",
      "Epoch: 1384 finished with training loss 81136.1767578125\n",
      "Epoch: 1385 finished with training loss 81135.8228515625\n",
      "Epoch: 1386 finished with training loss 81135.16796875\n",
      "Epoch: 1387 finished with training loss 81133.9134765625\n",
      "Epoch: 1388 finished with training loss 81133.02734375\n",
      "Epoch: 1389 finished with training loss 81132.8025390625\n",
      "Epoch: 1390 finished with training loss 81132.4412109375\n",
      "Epoch: 1391 finished with training loss 81132.04140625\n",
      "Epoch: 1392 finished with training loss 81131.6501953125\n",
      "Epoch: 1393 finished with training loss 81131.063671875\n",
      "Epoch: 1394 finished with training loss 81130.5330078125\n",
      "Epoch: 1395 finished with training loss 81130.533203125\n",
      "Epoch: 1396 finished with training loss 81130.1087890625\n",
      "Epoch: 1397 finished with training loss 81129.4271484375\n",
      "Epoch: 1398 finished with training loss 81129.4853515625\n",
      "Epoch: 1399 finished with training loss 81129.2185546875\n",
      "Epoch: 1400 finished with training loss 81128.9171875\n",
      "Epoch: 1401 finished with training loss 81128.3892578125\n",
      "Epoch: 1402 finished with training loss 81128.0728515625\n",
      "Epoch: 1403 finished with training loss 81127.6939453125\n",
      "Epoch: 1404 finished with training loss 81127.597265625\n",
      "Epoch: 1405 finished with training loss 81127.1794921875\n",
      "Epoch: 1406 finished with training loss 81126.7638671875\n",
      "Epoch: 1407 finished with training loss 81126.4955078125\n",
      "Epoch: 1408 finished with training loss 81126.21640625\n",
      "Epoch: 1409 finished with training loss 81125.8525390625\n",
      "Epoch: 1410 finished with training loss 81125.3673828125\n",
      "Epoch: 1411 finished with training loss 81125.26015625\n",
      "Epoch: 1412 finished with training loss 81124.9892578125\n",
      "Epoch: 1413 finished with training loss 81124.4818359375\n",
      "Epoch: 1414 finished with training loss 81124.1404296875\n",
      "Epoch: 1415 finished with training loss 81123.8501953125\n",
      "Epoch: 1416 finished with training loss 81123.624609375\n",
      "Epoch: 1417 finished with training loss 81123.2529296875\n",
      "Epoch: 1418 finished with training loss 81122.782421875\n",
      "Epoch: 1419 finished with training loss 81122.523828125\n",
      "Epoch: 1420 finished with training loss 81122.1953125\n",
      "Epoch: 1421 finished with training loss 81121.663671875\n",
      "Epoch: 1422 finished with training loss 81121.4408203125\n",
      "Epoch: 1423 finished with training loss 81121.2400390625\n",
      "Epoch: 1424 finished with training loss 81121.0466796875\n",
      "Epoch: 1425 finished with training loss 81120.387109375\n",
      "Epoch: 1426 finished with training loss 81119.9517578125\n",
      "Epoch: 1427 finished with training loss 81119.5673828125\n",
      "Epoch: 1428 finished with training loss 81119.3521484375\n",
      "Epoch: 1429 finished with training loss 81118.9935546875\n",
      "Epoch: 1430 finished with training loss 81118.67109375\n",
      "Epoch: 1431 finished with training loss 81118.31640625\n",
      "Epoch: 1432 finished with training loss 81117.9828125\n",
      "Epoch: 1433 finished with training loss 81117.5640625\n",
      "Epoch: 1434 finished with training loss 81117.189453125\n",
      "Epoch: 1435 finished with training loss 81116.8134765625\n",
      "Epoch: 1436 finished with training loss 81116.1119140625\n",
      "Epoch: 1437 finished with training loss 81115.9767578125\n",
      "Epoch: 1438 finished with training loss 81115.5818359375\n",
      "Epoch: 1439 finished with training loss 81114.8716796875\n",
      "Epoch: 1440 finished with training loss 81114.579296875\n",
      "Epoch: 1441 finished with training loss 81114.23671875\n",
      "Epoch: 1442 finished with training loss 81113.9734375\n",
      "Epoch: 1443 finished with training loss 81113.6427734375\n",
      "Epoch: 1444 finished with training loss 81113.4751953125\n",
      "Epoch: 1445 finished with training loss 81112.88125\n",
      "Epoch: 1446 finished with training loss 81112.7234375\n",
      "Epoch: 1447 finished with training loss 81112.494921875\n",
      "Epoch: 1448 finished with training loss 81112.0390625\n",
      "Epoch: 1449 finished with training loss 81111.6291015625\n",
      "Epoch: 1450 finished with training loss 81111.583203125\n",
      "Epoch: 1451 finished with training loss 81111.0330078125\n",
      "Epoch: 1452 finished with training loss 81110.66640625\n",
      "Epoch: 1453 finished with training loss 81110.530078125\n",
      "Epoch: 1454 finished with training loss 81109.980078125\n",
      "Epoch: 1455 finished with training loss 81109.487109375\n",
      "Epoch: 1456 finished with training loss 81109.176171875\n",
      "Epoch: 1457 finished with training loss 81108.8009765625\n",
      "Epoch: 1458 finished with training loss 81108.39296875\n",
      "Epoch: 1459 finished with training loss 81108.0787109375\n",
      "Epoch: 1460 finished with training loss 81107.7279296875\n",
      "Epoch: 1461 finished with training loss 81107.3796875\n",
      "Epoch: 1462 finished with training loss 81107.1224609375\n",
      "Epoch: 1463 finished with training loss 81106.6693359375\n",
      "Epoch: 1464 finished with training loss 81106.3763671875\n",
      "Epoch: 1465 finished with training loss 81106.1966796875\n",
      "Epoch: 1466 finished with training loss 81105.9009765625\n",
      "Epoch: 1467 finished with training loss 81105.6916015625\n",
      "Epoch: 1468 finished with training loss 81105.2875\n",
      "Epoch: 1469 finished with training loss 81105.154296875\n",
      "Epoch: 1470 finished with training loss 81104.841015625\n",
      "Epoch: 1471 finished with training loss 81104.4603515625\n",
      "Epoch: 1472 finished with training loss 81103.9552734375\n",
      "Epoch: 1473 finished with training loss 81103.359765625\n",
      "Epoch: 1474 finished with training loss 81103.2646484375\n",
      "Epoch: 1475 finished with training loss 81102.8197265625\n",
      "Epoch: 1476 finished with training loss 81102.4564453125\n",
      "Epoch: 1477 finished with training loss 81102.0220703125\n",
      "Epoch: 1478 finished with training loss 81101.7\n",
      "Epoch: 1479 finished with training loss 81101.431640625\n",
      "Epoch: 1480 finished with training loss 81101.1146484375\n",
      "Epoch: 1481 finished with training loss 81100.5970703125\n",
      "Epoch: 1482 finished with training loss 81100.383203125\n",
      "Epoch: 1483 finished with training loss 81099.5734375\n",
      "Epoch: 1484 finished with training loss 81099.26015625\n",
      "Epoch: 1485 finished with training loss 81099.1181640625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1486 finished with training loss 81098.59765625\n",
      "Epoch: 1487 finished with training loss 81098.1392578125\n",
      "Epoch: 1488 finished with training loss 81097.9740234375\n",
      "Epoch: 1489 finished with training loss 81097.5830078125\n",
      "Epoch: 1490 finished with training loss 81097.350390625\n",
      "Epoch: 1491 finished with training loss 81097.0794921875\n",
      "Epoch: 1492 finished with training loss 81096.823828125\n",
      "Epoch: 1493 finished with training loss 81096.51953125\n",
      "Epoch: 1494 finished with training loss 81096.0380859375\n",
      "Epoch: 1495 finished with training loss 81095.9220703125\n",
      "Epoch: 1496 finished with training loss 81095.8646484375\n",
      "Epoch: 1497 finished with training loss 81095.5560546875\n",
      "Epoch: 1498 finished with training loss 81094.780078125\n",
      "Epoch: 1499 finished with training loss 81094.3009765625\n",
      "Epoch: 1500 finished with training loss 81093.7728515625\n",
      "Epoch: 1501 finished with training loss 81093.730078125\n",
      "Epoch: 1502 finished with training loss 81093.0861328125\n",
      "Epoch: 1503 finished with training loss 81092.998046875\n",
      "Epoch: 1504 finished with training loss 81092.592578125\n",
      "Epoch: 1505 finished with training loss 81092.15078125\n",
      "Epoch: 1506 finished with training loss 81091.5884765625\n",
      "Epoch: 1507 finished with training loss 81091.234765625\n",
      "Epoch: 1508 finished with training loss 81090.830859375\n",
      "Epoch: 1509 finished with training loss 81090.6814453125\n",
      "Epoch: 1510 finished with training loss 81090.4703125\n",
      "Epoch: 1511 finished with training loss 81090.1115234375\n",
      "Epoch: 1512 finished with training loss 81089.777734375\n",
      "Epoch: 1513 finished with training loss 81089.3802734375\n",
      "Epoch: 1514 finished with training loss 81089.2923828125\n",
      "Epoch: 1515 finished with training loss 81088.77734375\n",
      "Epoch: 1516 finished with training loss 81088.4892578125\n",
      "Epoch: 1517 finished with training loss 81088.2271484375\n",
      "Epoch: 1518 finished with training loss 81087.898046875\n",
      "Epoch: 1519 finished with training loss 81087.863671875\n",
      "Epoch: 1520 finished with training loss 81087.2259765625\n",
      "Epoch: 1521 finished with training loss 81086.6080078125\n",
      "Epoch: 1522 finished with training loss 81086.234375\n",
      "Epoch: 1523 finished with training loss 81085.879296875\n",
      "Epoch: 1524 finished with training loss 81085.6216796875\n",
      "Epoch: 1525 finished with training loss 81085.144921875\n",
      "Epoch: 1526 finished with training loss 81085.1466796875\n",
      "Epoch: 1527 finished with training loss 81085.0265625\n",
      "Epoch: 1528 finished with training loss 81084.3958984375\n",
      "Epoch: 1529 finished with training loss 81084.019140625\n",
      "Epoch: 1530 finished with training loss 81083.769921875\n",
      "Epoch: 1531 finished with training loss 81083.17265625\n",
      "Epoch: 1532 finished with training loss 81082.9525390625\n",
      "Epoch: 1533 finished with training loss 81082.7662109375\n",
      "Epoch: 1534 finished with training loss 81082.3509765625\n",
      "Epoch: 1535 finished with training loss 81081.9328125\n",
      "Epoch: 1536 finished with training loss 81081.44140625\n",
      "Epoch: 1537 finished with training loss 81081.163671875\n",
      "Epoch: 1538 finished with training loss 81080.7931640625\n",
      "Epoch: 1539 finished with training loss 81080.5376953125\n",
      "Epoch: 1540 finished with training loss 81080.2291015625\n",
      "Epoch: 1541 finished with training loss 81079.89921875\n",
      "Epoch: 1542 finished with training loss 81079.4638671875\n",
      "Epoch: 1543 finished with training loss 81079.0384765625\n",
      "Epoch: 1544 finished with training loss 81078.8818359375\n",
      "Epoch: 1545 finished with training loss 81078.634765625\n",
      "Epoch: 1546 finished with training loss 81078.354296875\n",
      "Epoch: 1547 finished with training loss 81077.725\n",
      "Epoch: 1548 finished with training loss 81077.396484375\n",
      "Epoch: 1549 finished with training loss 81077.1501953125\n",
      "Epoch: 1550 finished with training loss 81076.903125\n",
      "Epoch: 1551 finished with training loss 81076.8091796875\n",
      "Epoch: 1552 finished with training loss 81076.8912109375\n",
      "Epoch: 1553 finished with training loss 81075.95078125\n",
      "Epoch: 1554 finished with training loss 81075.447265625\n",
      "Epoch: 1555 finished with training loss 81075.51796875\n",
      "Epoch: 1556 finished with training loss 81075.1197265625\n",
      "Epoch: 1557 finished with training loss 81074.9689453125\n",
      "Epoch: 1558 finished with training loss 81074.8896484375\n",
      "Epoch: 1559 finished with training loss 81074.351953125\n",
      "Epoch: 1560 finished with training loss 81073.8130859375\n",
      "Epoch: 1561 finished with training loss 81073.6703125\n",
      "Epoch: 1562 finished with training loss 81073.0712890625\n",
      "Epoch: 1563 finished with training loss 81072.5271484375\n",
      "Epoch: 1564 finished with training loss 81072.341796875\n",
      "Epoch: 1565 finished with training loss 81071.7095703125\n",
      "Epoch: 1566 finished with training loss 81071.2021484375\n",
      "Epoch: 1567 finished with training loss 81070.85234375\n",
      "Epoch: 1568 finished with training loss 81070.6322265625\n",
      "Epoch: 1569 finished with training loss 81070.24375\n",
      "Epoch: 1570 finished with training loss 81069.705078125\n",
      "Epoch: 1571 finished with training loss 81069.2771484375\n",
      "Epoch: 1572 finished with training loss 81068.883984375\n",
      "Epoch: 1573 finished with training loss 81068.4349609375\n",
      "Epoch: 1574 finished with training loss 81067.9484375\n",
      "Epoch: 1575 finished with training loss 81067.7029296875\n",
      "Epoch: 1576 finished with training loss 81067.5826171875\n",
      "Epoch: 1577 finished with training loss 81067.084765625\n",
      "Epoch: 1578 finished with training loss 81066.944140625\n",
      "Epoch: 1579 finished with training loss 81066.3857421875\n",
      "Epoch: 1580 finished with training loss 81066.1806640625\n",
      "Epoch: 1581 finished with training loss 81065.887109375\n",
      "Epoch: 1582 finished with training loss 81065.4908203125\n",
      "Epoch: 1583 finished with training loss 81064.9462890625\n",
      "Epoch: 1584 finished with training loss 81064.5439453125\n",
      "Epoch: 1585 finished with training loss 81064.3447265625\n",
      "Epoch: 1586 finished with training loss 81064.1482421875\n",
      "Epoch: 1587 finished with training loss 81063.63359375\n",
      "Epoch: 1588 finished with training loss 81063.1724609375\n",
      "Epoch: 1589 finished with training loss 81063.1146484375\n",
      "Epoch: 1590 finished with training loss 81062.4921875\n",
      "Epoch: 1591 finished with training loss 81062.283984375\n",
      "Epoch: 1592 finished with training loss 81062.114453125\n",
      "Epoch: 1593 finished with training loss 81061.3853515625\n",
      "Epoch: 1594 finished with training loss 81060.8150390625\n",
      "Epoch: 1595 finished with training loss 81060.50703125\n",
      "Epoch: 1596 finished with training loss 81060.161328125\n",
      "Epoch: 1597 finished with training loss 81059.6705078125\n",
      "Epoch: 1598 finished with training loss 81059.43203125\n",
      "Epoch: 1599 finished with training loss 81058.9240234375\n",
      "Epoch: 1600 finished with training loss 81058.6181640625\n",
      "Epoch: 1601 finished with training loss 81058.266015625\n",
      "Epoch: 1602 finished with training loss 81057.76796875\n",
      "Epoch: 1603 finished with training loss 81057.2767578125\n",
      "Epoch: 1604 finished with training loss 81056.6056640625\n",
      "Epoch: 1605 finished with training loss 81056.31484375\n",
      "Epoch: 1606 finished with training loss 81055.939453125\n",
      "Epoch: 1607 finished with training loss 81055.6033203125\n",
      "Epoch: 1608 finished with training loss 81055.3853515625\n",
      "Epoch: 1609 finished with training loss 81055.106640625\n",
      "Epoch: 1610 finished with training loss 81054.5708984375\n",
      "Epoch: 1611 finished with training loss 81054.141796875\n",
      "Epoch: 1612 finished with training loss 81053.6712890625\n",
      "Epoch: 1613 finished with training loss 81053.311328125\n",
      "Epoch: 1614 finished with training loss 81052.9802734375\n",
      "Epoch: 1615 finished with training loss 81052.4544921875\n",
      "Epoch: 1616 finished with training loss 81052.138671875\n",
      "Epoch: 1617 finished with training loss 81051.7857421875\n",
      "Epoch: 1618 finished with training loss 81051.4923828125\n",
      "Epoch: 1619 finished with training loss 81051.099609375\n",
      "Epoch: 1620 finished with training loss 81050.45078125\n",
      "Epoch: 1621 finished with training loss 81050.259375\n",
      "Epoch: 1622 finished with training loss 81049.830078125\n",
      "Epoch: 1623 finished with training loss 81049.7037109375\n",
      "Epoch: 1624 finished with training loss 81049.3986328125\n",
      "Epoch: 1625 finished with training loss 81048.8306640625\n",
      "Epoch: 1626 finished with training loss 81048.421875\n",
      "Epoch: 1627 finished with training loss 81048.2806640625\n",
      "Epoch: 1628 finished with training loss 81047.61640625\n",
      "Epoch: 1629 finished with training loss 81047.57890625\n",
      "Epoch: 1630 finished with training loss 81046.841796875\n",
      "Epoch: 1631 finished with training loss 81046.65546875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1632 finished with training loss 81046.19296875\n",
      "Epoch: 1633 finished with training loss 81046.209765625\n",
      "Epoch: 1634 finished with training loss 81045.37578125\n",
      "Epoch: 1635 finished with training loss 81044.987890625\n",
      "Epoch: 1636 finished with training loss 81044.8986328125\n",
      "Epoch: 1637 finished with training loss 81044.429296875\n",
      "Epoch: 1638 finished with training loss 81043.9466796875\n",
      "Epoch: 1639 finished with training loss 81043.419921875\n",
      "Epoch: 1640 finished with training loss 81043.0169921875\n",
      "Epoch: 1641 finished with training loss 81042.7267578125\n",
      "Epoch: 1642 finished with training loss 81042.405078125\n",
      "Epoch: 1643 finished with training loss 81041.9810546875\n",
      "Epoch: 1644 finished with training loss 81041.6759765625\n",
      "Epoch: 1645 finished with training loss 81041.4208984375\n",
      "Epoch: 1646 finished with training loss 81040.9623046875\n",
      "Epoch: 1647 finished with training loss 81040.6728515625\n",
      "Epoch: 1648 finished with training loss 81040.306640625\n",
      "Epoch: 1649 finished with training loss 81039.4994140625\n",
      "Epoch: 1650 finished with training loss 81039.24296875\n",
      "Epoch: 1651 finished with training loss 81038.625390625\n",
      "Epoch: 1652 finished with training loss 81038.2392578125\n",
      "Epoch: 1653 finished with training loss 81037.8314453125\n",
      "Epoch: 1654 finished with training loss 81037.426171875\n",
      "Epoch: 1655 finished with training loss 81037.1455078125\n",
      "Epoch: 1656 finished with training loss 81037.035546875\n",
      "Epoch: 1657 finished with training loss 81036.60078125\n",
      "Epoch: 1658 finished with training loss 81036.31953125\n",
      "Epoch: 1659 finished with training loss 81035.633203125\n",
      "Epoch: 1660 finished with training loss 81035.2765625\n",
      "Epoch: 1661 finished with training loss 81034.7625\n",
      "Epoch: 1662 finished with training loss 81034.6787109375\n",
      "Epoch: 1663 finished with training loss 81033.9681640625\n",
      "Epoch: 1664 finished with training loss 81033.4619140625\n",
      "Epoch: 1665 finished with training loss 81033.06953125\n",
      "Epoch: 1666 finished with training loss 81032.7279296875\n",
      "Epoch: 1667 finished with training loss 81032.2880859375\n",
      "Epoch: 1668 finished with training loss 81032.040625\n",
      "Epoch: 1669 finished with training loss 81031.6505859375\n",
      "Epoch: 1670 finished with training loss 81031.4041015625\n",
      "Epoch: 1671 finished with training loss 81031.0732421875\n",
      "Epoch: 1672 finished with training loss 81030.743359375\n",
      "Epoch: 1673 finished with training loss 81030.4345703125\n",
      "Epoch: 1674 finished with training loss 81029.9349609375\n",
      "Epoch: 1675 finished with training loss 81029.7234375\n",
      "Epoch: 1676 finished with training loss 81029.048828125\n",
      "Epoch: 1677 finished with training loss 81028.475390625\n",
      "Epoch: 1678 finished with training loss 81028.1013671875\n",
      "Epoch: 1679 finished with training loss 81027.6126953125\n",
      "Epoch: 1680 finished with training loss 81027.5181640625\n",
      "Epoch: 1681 finished with training loss 81027.25\n",
      "Epoch: 1682 finished with training loss 81026.6603515625\n",
      "Epoch: 1683 finished with training loss 81026.5205078125\n",
      "Epoch: 1684 finished with training loss 81026.2876953125\n",
      "Epoch: 1685 finished with training loss 81026.221484375\n",
      "Epoch: 1686 finished with training loss 81025.5775390625\n",
      "Epoch: 1687 finished with training loss 81025.1564453125\n",
      "Epoch: 1688 finished with training loss 81024.994140625\n",
      "Epoch: 1689 finished with training loss 81024.4708984375\n",
      "Epoch: 1690 finished with training loss 81024.1013671875\n",
      "Epoch: 1691 finished with training loss 81023.3421875\n",
      "Epoch: 1692 finished with training loss 81022.8765625\n",
      "Epoch: 1693 finished with training loss 81022.563671875\n",
      "Epoch: 1694 finished with training loss 81022.4044921875\n",
      "Epoch: 1695 finished with training loss 81021.842578125\n",
      "Epoch: 1696 finished with training loss 81021.7078125\n",
      "Epoch: 1697 finished with training loss 81021.2431640625\n",
      "Epoch: 1698 finished with training loss 81020.6205078125\n",
      "Epoch: 1699 finished with training loss 81020.3970703125\n",
      "Epoch: 1700 finished with training loss 81019.993359375\n",
      "Epoch: 1701 finished with training loss 81019.6294921875\n",
      "Epoch: 1702 finished with training loss 81019.5275390625\n",
      "Epoch: 1703 finished with training loss 81018.8224609375\n",
      "Epoch: 1704 finished with training loss 81018.56328125\n",
      "Epoch: 1705 finished with training loss 81018.3376953125\n",
      "Epoch: 1706 finished with training loss 81017.8353515625\n",
      "Epoch: 1707 finished with training loss 81016.8953125\n",
      "Epoch: 1708 finished with training loss 81016.8291015625\n",
      "Epoch: 1709 finished with training loss 81016.6478515625\n",
      "Epoch: 1710 finished with training loss 81016.544140625\n",
      "Epoch: 1711 finished with training loss 81016.348828125\n",
      "Epoch: 1712 finished with training loss 81015.666015625\n",
      "Epoch: 1713 finished with training loss 81015.314453125\n",
      "Epoch: 1714 finished with training loss 81015.0767578125\n",
      "Epoch: 1715 finished with training loss 81014.71953125\n",
      "Epoch: 1716 finished with training loss 81014.2947265625\n",
      "Epoch: 1717 finished with training loss 81013.5576171875\n",
      "Epoch: 1718 finished with training loss 81013.2734375\n",
      "Epoch: 1719 finished with training loss 81012.8890625\n",
      "Epoch: 1720 finished with training loss 81012.634375\n",
      "Epoch: 1721 finished with training loss 81012.679296875\n",
      "Epoch: 1722 finished with training loss 81011.9951171875\n",
      "Epoch: 1723 finished with training loss 81011.7451171875\n",
      "Epoch: 1724 finished with training loss 81011.2185546875\n",
      "Epoch: 1725 finished with training loss 81010.85078125\n",
      "Epoch: 1726 finished with training loss 81010.3369140625\n",
      "Epoch: 1727 finished with training loss 81009.8685546875\n",
      "Epoch: 1728 finished with training loss 81009.45\n",
      "Epoch: 1729 finished with training loss 81009.02265625\n",
      "Epoch: 1730 finished with training loss 81008.7123046875\n",
      "Epoch: 1731 finished with training loss 81008.6466796875\n",
      "Epoch: 1732 finished with training loss 81007.997265625\n",
      "Epoch: 1733 finished with training loss 81007.2890625\n",
      "Epoch: 1734 finished with training loss 81007.003125\n",
      "Epoch: 1735 finished with training loss 81006.3654296875\n",
      "Epoch: 1736 finished with training loss 81005.915234375\n",
      "Epoch: 1737 finished with training loss 81005.7251953125\n",
      "Epoch: 1738 finished with training loss 81005.2642578125\n",
      "Epoch: 1739 finished with training loss 81004.929296875\n",
      "Epoch: 1740 finished with training loss 81004.6556640625\n",
      "Epoch: 1741 finished with training loss 81004.121484375\n",
      "Epoch: 1742 finished with training loss 81003.8025390625\n",
      "Epoch: 1743 finished with training loss 81003.5455078125\n",
      "Epoch: 1744 finished with training loss 81003.1869140625\n",
      "Epoch: 1745 finished with training loss 81002.7763671875\n",
      "Epoch: 1746 finished with training loss 81002.514453125\n",
      "Epoch: 1747 finished with training loss 81002.161328125\n",
      "Epoch: 1748 finished with training loss 81001.4498046875\n",
      "Epoch: 1749 finished with training loss 81001.2564453125\n",
      "Epoch: 1750 finished with training loss 81000.909375\n",
      "Epoch: 1751 finished with training loss 81000.522265625\n",
      "Epoch: 1752 finished with training loss 80999.9587890625\n",
      "Epoch: 1753 finished with training loss 80999.6408203125\n",
      "Epoch: 1754 finished with training loss 80999.274609375\n",
      "Epoch: 1755 finished with training loss 80998.87265625\n",
      "Epoch: 1756 finished with training loss 80998.602734375\n",
      "Epoch: 1757 finished with training loss 80998.1908203125\n",
      "Epoch: 1758 finished with training loss 80997.823046875\n",
      "Epoch: 1759 finished with training loss 80997.1921875\n",
      "Epoch: 1760 finished with training loss 80996.850390625\n",
      "Epoch: 1761 finished with training loss 80996.5220703125\n",
      "Epoch: 1762 finished with training loss 80996.236328125\n",
      "Epoch: 1763 finished with training loss 80995.6345703125\n",
      "Epoch: 1764 finished with training loss 80995.38125\n",
      "Epoch: 1765 finished with training loss 80995.1267578125\n",
      "Epoch: 1766 finished with training loss 80994.5400390625\n",
      "Epoch: 1767 finished with training loss 80994.3064453125\n",
      "Epoch: 1768 finished with training loss 80994.02421875\n",
      "Epoch: 1769 finished with training loss 80993.7107421875\n",
      "Epoch: 1770 finished with training loss 80993.3759765625\n",
      "Epoch: 1771 finished with training loss 80993.1146484375\n",
      "Epoch: 1772 finished with training loss 80992.8177734375\n",
      "Epoch: 1773 finished with training loss 80992.10703125\n",
      "Epoch: 1774 finished with training loss 80991.484765625\n",
      "Epoch: 1775 finished with training loss 80990.8548828125\n",
      "Epoch: 1776 finished with training loss 80990.468359375\n",
      "Epoch: 1777 finished with training loss 80990.1181640625\n",
      "Epoch: 1778 finished with training loss 80989.715234375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1779 finished with training loss 80989.3734375\n",
      "Epoch: 1780 finished with training loss 80988.8166015625\n",
      "Epoch: 1781 finished with training loss 80988.5244140625\n",
      "Epoch: 1782 finished with training loss 80988.21796875\n",
      "Epoch: 1783 finished with training loss 80987.29296875\n",
      "Epoch: 1784 finished with training loss 80987.27109375\n",
      "Epoch: 1785 finished with training loss 80986.743359375\n",
      "Epoch: 1786 finished with training loss 80986.0595703125\n",
      "Epoch: 1787 finished with training loss 80985.6134765625\n",
      "Epoch: 1788 finished with training loss 80985.09140625\n",
      "Epoch: 1789 finished with training loss 80984.7326171875\n",
      "Epoch: 1790 finished with training loss 80984.1818359375\n",
      "Epoch: 1791 finished with training loss 80984.0736328125\n",
      "Epoch: 1792 finished with training loss 80983.6251953125\n",
      "Epoch: 1793 finished with training loss 80983.1533203125\n",
      "Epoch: 1794 finished with training loss 80982.6056640625\n",
      "Epoch: 1795 finished with training loss 80982.3544921875\n",
      "Epoch: 1796 finished with training loss 80982.064453125\n",
      "Epoch: 1797 finished with training loss 80981.1775390625\n",
      "Epoch: 1798 finished with training loss 80980.7998046875\n",
      "Epoch: 1799 finished with training loss 80980.2083984375\n",
      "Epoch: 1800 finished with training loss 80979.772265625\n",
      "Epoch: 1801 finished with training loss 80979.6869140625\n",
      "Epoch: 1802 finished with training loss 80979.214453125\n",
      "Epoch: 1803 finished with training loss 80978.8162109375\n",
      "Epoch: 1804 finished with training loss 80978.5775390625\n",
      "Epoch: 1805 finished with training loss 80978.35\n",
      "Epoch: 1806 finished with training loss 80977.6810546875\n",
      "Epoch: 1807 finished with training loss 80977.37734375\n",
      "Epoch: 1808 finished with training loss 80976.85\n",
      "Epoch: 1809 finished with training loss 80976.4033203125\n",
      "Epoch: 1810 finished with training loss 80975.9361328125\n",
      "Epoch: 1811 finished with training loss 80975.498046875\n",
      "Epoch: 1812 finished with training loss 80975.3109375\n",
      "Epoch: 1813 finished with training loss 80974.7884765625\n",
      "Epoch: 1814 finished with training loss 80974.51484375\n",
      "Epoch: 1815 finished with training loss 80974.2205078125\n",
      "Epoch: 1816 finished with training loss 80973.831640625\n",
      "Epoch: 1817 finished with training loss 80973.1890625\n",
      "Epoch: 1818 finished with training loss 80972.8751953125\n",
      "Epoch: 1819 finished with training loss 80972.1888671875\n",
      "Epoch: 1820 finished with training loss 80972.133984375\n",
      "Epoch: 1821 finished with training loss 80971.4775390625\n",
      "Epoch: 1822 finished with training loss 80970.85859375\n",
      "Epoch: 1823 finished with training loss 80970.38671875\n",
      "Epoch: 1824 finished with training loss 80969.8130859375\n",
      "Epoch: 1825 finished with training loss 80969.3236328125\n",
      "Epoch: 1826 finished with training loss 80968.8798828125\n",
      "Epoch: 1827 finished with training loss 80968.81875\n",
      "Epoch: 1828 finished with training loss 80968.1712890625\n",
      "Epoch: 1829 finished with training loss 80967.456640625\n",
      "Epoch: 1830 finished with training loss 80966.69609375\n",
      "Epoch: 1831 finished with training loss 80966.0755859375\n",
      "Epoch: 1832 finished with training loss 80965.4990234375\n",
      "Epoch: 1833 finished with training loss 80964.8423828125\n",
      "Epoch: 1834 finished with training loss 80964.218359375\n",
      "Epoch: 1835 finished with training loss 80963.8841796875\n",
      "Epoch: 1836 finished with training loss 80963.5154296875\n",
      "Epoch: 1837 finished with training loss 80962.8353515625\n",
      "Epoch: 1838 finished with training loss 80962.5650390625\n",
      "Epoch: 1839 finished with training loss 80961.988671875\n",
      "Epoch: 1840 finished with training loss 80961.5984375\n",
      "Epoch: 1841 finished with training loss 80960.9775390625\n",
      "Epoch: 1842 finished with training loss 80960.5787109375\n",
      "Epoch: 1843 finished with training loss 80960.1841796875\n",
      "Epoch: 1844 finished with training loss 80959.6185546875\n",
      "Epoch: 1845 finished with training loss 80959.5296875\n",
      "Epoch: 1846 finished with training loss 80959.15390625\n",
      "Epoch: 1847 finished with training loss 80958.6990234375\n",
      "Epoch: 1848 finished with training loss 80958.0734375\n",
      "Epoch: 1849 finished with training loss 80957.6822265625\n",
      "Epoch: 1850 finished with training loss 80957.034375\n",
      "Epoch: 1851 finished with training loss 80956.7916015625\n",
      "Epoch: 1852 finished with training loss 80956.541796875\n",
      "Epoch: 1853 finished with training loss 80956.018359375\n",
      "Epoch: 1854 finished with training loss 80955.5373046875\n",
      "Epoch: 1855 finished with training loss 80954.9388671875\n",
      "Epoch: 1856 finished with training loss 80954.4939453125\n",
      "Epoch: 1857 finished with training loss 80954.109375\n",
      "Epoch: 1858 finished with training loss 80953.9103515625\n",
      "Epoch: 1859 finished with training loss 80953.3232421875\n",
      "Epoch: 1860 finished with training loss 80952.630078125\n",
      "Epoch: 1861 finished with training loss 80952.44296875\n",
      "Epoch: 1862 finished with training loss 80951.998046875\n",
      "Epoch: 1863 finished with training loss 80951.48203125\n",
      "Epoch: 1864 finished with training loss 80951.0181640625\n",
      "Epoch: 1865 finished with training loss 80951.2587890625\n",
      "Epoch: 1866 finished with training loss 80950.3919921875\n",
      "Epoch: 1867 finished with training loss 80950.3404296875\n",
      "Epoch: 1868 finished with training loss 80950.01015625\n",
      "Epoch: 1869 finished with training loss 80949.055078125\n",
      "Epoch: 1870 finished with training loss 80948.693359375\n",
      "Epoch: 1871 finished with training loss 80948.0927734375\n",
      "Epoch: 1872 finished with training loss 80947.67109375\n",
      "Epoch: 1873 finished with training loss 80947.419140625\n",
      "Epoch: 1874 finished with training loss 80946.809375\n",
      "Epoch: 1875 finished with training loss 80946.3943359375\n",
      "Epoch: 1876 finished with training loss 80946.1361328125\n",
      "Epoch: 1877 finished with training loss 80945.41171875\n",
      "Epoch: 1878 finished with training loss 80945.1810546875\n",
      "Epoch: 1879 finished with training loss 80944.4513671875\n",
      "Epoch: 1880 finished with training loss 80944.3046875\n",
      "Epoch: 1881 finished with training loss 80944.1185546875\n",
      "Epoch: 1882 finished with training loss 80943.1986328125\n",
      "Epoch: 1883 finished with training loss 80942.7009765625\n",
      "Epoch: 1884 finished with training loss 80942.2498046875\n",
      "Epoch: 1885 finished with training loss 80941.85703125\n",
      "Epoch: 1886 finished with training loss 80941.3142578125\n",
      "Epoch: 1887 finished with training loss 80940.7609375\n",
      "Epoch: 1888 finished with training loss 80940.4080078125\n",
      "Epoch: 1889 finished with training loss 80940.202734375\n",
      "Epoch: 1890 finished with training loss 80939.80234375\n",
      "Epoch: 1891 finished with training loss 80939.341796875\n",
      "Epoch: 1892 finished with training loss 80938.9794921875\n",
      "Epoch: 1893 finished with training loss 80938.340234375\n",
      "Epoch: 1894 finished with training loss 80937.6984375\n",
      "Epoch: 1895 finished with training loss 80937.1970703125\n",
      "Epoch: 1896 finished with training loss 80936.6248046875\n",
      "Epoch: 1897 finished with training loss 80936.3087890625\n",
      "Epoch: 1898 finished with training loss 80935.9076171875\n",
      "Epoch: 1899 finished with training loss 80935.21484375\n",
      "Epoch: 1900 finished with training loss 80934.8697265625\n",
      "Epoch: 1901 finished with training loss 80934.289453125\n",
      "Epoch: 1902 finished with training loss 80934.123046875\n",
      "Epoch: 1903 finished with training loss 80934.006640625\n",
      "Epoch: 1904 finished with training loss 80933.3779296875\n",
      "Epoch: 1905 finished with training loss 80932.9283203125\n",
      "Epoch: 1906 finished with training loss 80932.3189453125\n",
      "Epoch: 1907 finished with training loss 80932.022265625\n",
      "Epoch: 1908 finished with training loss 80931.6482421875\n",
      "Epoch: 1909 finished with training loss 80931.1078125\n",
      "Epoch: 1910 finished with training loss 80930.67109375\n",
      "Epoch: 1911 finished with training loss 80930.062109375\n",
      "Epoch: 1912 finished with training loss 80929.6359375\n",
      "Epoch: 1913 finished with training loss 80929.133984375\n",
      "Epoch: 1914 finished with training loss 80928.7361328125\n",
      "Epoch: 1915 finished with training loss 80928.229296875\n",
      "Epoch: 1916 finished with training loss 80927.83359375\n",
      "Epoch: 1917 finished with training loss 80927.0630859375\n",
      "Epoch: 1918 finished with training loss 80926.9966796875\n",
      "Epoch: 1919 finished with training loss 80926.610546875\n",
      "Epoch: 1920 finished with training loss 80926.2302734375\n",
      "Epoch: 1921 finished with training loss 80925.845703125\n",
      "Epoch: 1922 finished with training loss 80925.3904296875\n",
      "Epoch: 1923 finished with training loss 80924.9490234375\n",
      "Epoch: 1924 finished with training loss 80924.3189453125\n",
      "Epoch: 1925 finished with training loss 80923.8462890625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1926 finished with training loss 80923.208984375\n",
      "Epoch: 1927 finished with training loss 80922.496484375\n",
      "Epoch: 1928 finished with training loss 80921.9416015625\n",
      "Epoch: 1929 finished with training loss 80921.4419921875\n",
      "Epoch: 1930 finished with training loss 80920.9853515625\n",
      "Epoch: 1931 finished with training loss 80920.4015625\n",
      "Epoch: 1932 finished with training loss 80920.14140625\n",
      "Epoch: 1933 finished with training loss 80920.0125\n",
      "Epoch: 1934 finished with training loss 80919.38046875\n",
      "Epoch: 1935 finished with training loss 80918.810546875\n",
      "Epoch: 1936 finished with training loss 80918.540625\n",
      "Epoch: 1937 finished with training loss 80917.9134765625\n",
      "Epoch: 1938 finished with training loss 80917.5197265625\n",
      "Epoch: 1939 finished with training loss 80916.81015625\n",
      "Epoch: 1940 finished with training loss 80916.6607421875\n",
      "Epoch: 1941 finished with training loss 80916.3958984375\n",
      "Epoch: 1942 finished with training loss 80915.9763671875\n",
      "Epoch: 1943 finished with training loss 80915.5966796875\n",
      "Epoch: 1944 finished with training loss 80915.1490234375\n",
      "Epoch: 1945 finished with training loss 80914.76484375\n",
      "Epoch: 1946 finished with training loss 80914.1451171875\n",
      "Epoch: 1947 finished with training loss 80913.8677734375\n",
      "Epoch: 1948 finished with training loss 80913.29375\n",
      "Epoch: 1949 finished with training loss 80912.693359375\n",
      "Epoch: 1950 finished with training loss 80912.221484375\n",
      "Epoch: 1951 finished with training loss 80911.491015625\n",
      "Epoch: 1952 finished with training loss 80911.179296875\n",
      "Epoch: 1953 finished with training loss 80911.16484375\n",
      "Epoch: 1954 finished with training loss 80910.24296875\n",
      "Epoch: 1955 finished with training loss 80909.626171875\n",
      "Epoch: 1956 finished with training loss 80909.2\n",
      "Epoch: 1957 finished with training loss 80908.4623046875\n",
      "Epoch: 1958 finished with training loss 80908.1509765625\n",
      "Epoch: 1959 finished with training loss 80907.56875\n",
      "Epoch: 1960 finished with training loss 80906.867578125\n",
      "Epoch: 1961 finished with training loss 80906.4314453125\n",
      "Epoch: 1962 finished with training loss 80906.1171875\n",
      "Epoch: 1963 finished with training loss 80905.669140625\n",
      "Epoch: 1964 finished with training loss 80905.044140625\n",
      "Epoch: 1965 finished with training loss 80904.63984375\n",
      "Epoch: 1966 finished with training loss 80904.1591796875\n",
      "Epoch: 1967 finished with training loss 80903.4451171875\n",
      "Epoch: 1968 finished with training loss 80903.0654296875\n",
      "Epoch: 1969 finished with training loss 80902.7994140625\n",
      "Epoch: 1970 finished with training loss 80902.1103515625\n",
      "Epoch: 1971 finished with training loss 80901.7013671875\n",
      "Epoch: 1972 finished with training loss 80901.598828125\n",
      "Epoch: 1973 finished with training loss 80901.028125\n",
      "Epoch: 1974 finished with training loss 80900.4736328125\n",
      "Epoch: 1975 finished with training loss 80900.144921875\n",
      "Epoch: 1976 finished with training loss 80899.629296875\n",
      "Epoch: 1977 finished with training loss 80899.0384765625\n",
      "Epoch: 1978 finished with training loss 80898.606640625\n",
      "Epoch: 1979 finished with training loss 80898.0455078125\n",
      "Epoch: 1980 finished with training loss 80897.8646484375\n",
      "Epoch: 1981 finished with training loss 80897.0376953125\n",
      "Epoch: 1982 finished with training loss 80896.4919921875\n",
      "Epoch: 1983 finished with training loss 80895.80390625\n",
      "Epoch: 1984 finished with training loss 80895.323046875\n",
      "Epoch: 1985 finished with training loss 80894.826171875\n",
      "Epoch: 1986 finished with training loss 80894.4125\n",
      "Epoch: 1987 finished with training loss 80894.149609375\n",
      "Epoch: 1988 finished with training loss 80893.675390625\n",
      "Epoch: 1989 finished with training loss 80893.257421875\n",
      "Epoch: 1990 finished with training loss 80892.680859375\n",
      "Epoch: 1991 finished with training loss 80891.9998046875\n",
      "Epoch: 1992 finished with training loss 80891.6419921875\n",
      "Epoch: 1993 finished with training loss 80891.065234375\n",
      "Epoch: 1994 finished with training loss 80890.7966796875\n",
      "Epoch: 1995 finished with training loss 80890.0423828125\n",
      "Epoch: 1996 finished with training loss 80889.5611328125\n",
      "Epoch: 1997 finished with training loss 80888.8029296875\n",
      "Epoch: 1998 finished with training loss 80888.35234375\n",
      "Epoch: 1999 finished with training loss 80887.744140625\n"
     ]
    }
   ],
   "source": [
    "net = getRegNet(device)\n",
    "\n",
    "\n",
    "optimizer = AdaHessian(net.parameters(), lr=0.1, weight_decay=5e-4)\n",
    "\n",
    "\n",
    "\n",
    "generateExperimentReg(net, optimizer, train_reg_loader, test_reg_loader, True, \n",
    "                      \"AdamHess_Reg_torch.csv\", device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
