# First Order and Second Order Optimizer Analysis


 - Authors: Haoran Pu, Yucen, Sun.

## Overview

This project is a comprehensive study between first order optimizers and second order optimizers in both theoretical and practical realms.

## Related Work

 - [First Order Gradient Descent Analysis](http://www.cs.columbia.edu/~andoni/advancedS21/materials.html)
 - [AdaHessian Implementation](https://github.com/davda54/ada-hessian)
 - [ADAHESSIAN: An Adaptive Second Order Optimizer for Machine Learning](https://arxiv.org/abs/2006.00719)
 - [The Space Complexity of Approximating the Frequency Moments](https://www.sciencedirect.com/science/article/pii/S0022000097915452)

## Project Decomposition

 - `DLsys_final_project_report.pdf`: Our final report.
 - `ada_hessian.py`: Adahessian Implementation for Experimentation.
 - `experiment-torch.ipynb`: Our experiment code, just run each block one by one and you should get the result. (Note: it is extremely slow)
 - `data`: This contains the experiment data based on our run, which are used for `presentation` and `final analysis`.